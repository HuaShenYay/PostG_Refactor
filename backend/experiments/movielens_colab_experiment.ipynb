{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "title"
      },
      "source": [
        "# MovieLens-100K Top-N 实验（Colab版，独立于现有 `.py`）\n",
        "\n",
        "本 notebook 是**全新实现**，用于在 Google Colab 上复现实验，不修改你项目中的 `backend/experiments/movielens_experiment.py`。\n",
        "\n",
        "对比方法：\n",
        "- CB（TF-IDF）\n",
        "- Item-CF\n",
        "- User-CF\n",
        "- BERT-Enhanced（Item-CF + User-CF + BERTopic向量）\n",
        "\n",
        "评估指标：Precision@5/10、Recall@5/10、Hit@5/10。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "install"
      },
      "outputs": [],
      "source": [
        "# Colab 依赖安装（首次运行）\n",
        "!pip -q install numpy pandas scikit-learn matplotlib tqdm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "imports"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import json\n",
        "import random\n",
        "import zipfile\n",
        "import urllib.request\n",
        "from dataclasses import dataclass\n",
        "from collections import defaultdict\n",
        "from datetime import datetime\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "print('Imports OK')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "config"
      },
      "outputs": [],
      "source": [
        "@dataclass\n",
        "class Config:\n",
        "    dataset_dir: str = './ml-100k'\n",
        "    test_ratio: float = 0.2\n",
        "    seed: int = 42\n",
        "    positive_threshold: float = 4.0\n",
        "    top_ks: tuple = (5, 10)\n",
        "\n",
        "cfg = Config()\n",
        "cfg"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "download"
      },
      "outputs": [],
      "source": [
        "def download_movielens_100k(dataset_dir='./ml-100k'):\n",
        "    if os.path.exists(dataset_dir):\n",
        "        print('Dataset exists:', dataset_dir)\n",
        "        return\n",
        "\n",
        "    url = 'https://files.grouplens.org/datasets/movielens/ml-100k.zip'\n",
        "    zip_path = './ml-100k.zip'\n",
        "    print('Downloading from', url)\n",
        "    urllib.request.urlretrieve(url, zip_path)\n",
        "    with zipfile.ZipFile(zip_path, 'r') as zf:\n",
        "        zf.extractall('./')\n",
        "    os.remove(zip_path)\n",
        "    print('Done')\n",
        "\n",
        "download_movielens_100k(cfg.dataset_dir)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "load"
      },
      "outputs": [],
      "source": [
        "def load_movies(dataset_dir):\n",
        "    genre_names = [\n",
        "        'unknown','Action','Adventure','Animation','Children','Comedy','Crime',\n",
        "        'Documentary','Drama','Fantasy','Film-Noir','Horror','Musical','Mystery',\n",
        "        'Romance','Sci-Fi','Thriller','War','Western'\n",
        "    ]\n",
        "\n",
        "    items = []\n",
        "    with open(os.path.join(dataset_dir, 'u.item'), 'r', encoding='latin-1') as f:\n",
        "        for line in f:\n",
        "            parts = line.strip().split('|')\n",
        "            movie_id = int(parts[0])\n",
        "            title = parts[1]\n",
        "            flags = parts[5:24]\n",
        "            genres = [g for g, v in zip(genre_names, flags) if v == '1']\n",
        "            content = f\"{title} {' '.join(genres)} movie film\"\n",
        "            items.append({'id': movie_id, 'title': title, 'genres': genres, 'content': content})\n",
        "    return items\n",
        "\n",
        "def load_ratings(dataset_dir):\n",
        "    out = []\n",
        "    with open(os.path.join(dataset_dir, 'u.data'), 'r', encoding='utf-8') as f:\n",
        "        for line in f:\n",
        "            uid, mid, rating, ts = line.strip().split('\\t')\n",
        "            out.append({\n",
        "                'user_id': int(uid),\n",
        "                'poem_id': int(mid),\n",
        "                'rating': float(rating),\n",
        "                'created_at': datetime.fromtimestamp(int(ts)),\n",
        "            })\n",
        "    return out\n",
        "\n",
        "movies = load_movies(cfg.dataset_dir)\n",
        "ratings = load_ratings(cfg.dataset_dir)\n",
        "print('movies=', len(movies), 'ratings=', len(ratings), 'users=', len(set(x['user_id'] for x in ratings)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "split"
      },
      "outputs": [],
      "source": [
        "def split_by_user_random(interactions, test_ratio=0.2, seed=42):\n",
        "    rng = random.Random(seed)\n",
        "    by_user = defaultdict(list)\n",
        "    for x in interactions:\n",
        "        by_user[x['user_id']].append(x)\n",
        "\n",
        "    train, test = [], []\n",
        "    for uid, xs in by_user.items():\n",
        "        xs = xs.copy()\n",
        "        rng.shuffle(xs)\n",
        "        n_test = max(1, int(len(xs) * test_ratio))\n",
        "        test.extend(xs[:n_test])\n",
        "        train.extend(xs[n_test:])\n",
        "    return train, test\n",
        "\n",
        "def build_user_index(interactions):\n",
        "    d = defaultdict(list)\n",
        "    for x in interactions:\n",
        "        d[x['user_id']].append(x)\n",
        "    return d\n",
        "\n",
        "train, test = split_by_user_random(ratings, cfg.test_ratio, cfg.seed)\n",
        "user_train = build_user_index(train)\n",
        "user_test = build_user_index(test)\n",
        "len(train), len(test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "models"
      },
      "outputs": [],
      "source": [
        "class ContentBasedRecommender:\n",
        "    def __init__(self):\n",
        "        self.vectorizer = TfidfVectorizer(stop_words='english', ngram_range=(1,2), max_features=8000)\n",
        "        self.items = None\n",
        "        self.item_vectors = None\n",
        "        self.id_to_idx = {}\n",
        "\n",
        "    def fit(self, items):\n",
        "        self.items = items\n",
        "        self.id_to_idx = {x['id']: i for i, x in enumerate(items)}\n",
        "        self.item_vectors = self.vectorizer.fit_transform([x['content'] for x in items])\n",
        "\n",
        "    def recommend(self, user_interactions, exclude_ids, top_k=10):\n",
        "        if self.item_vectors is None:\n",
        "            return []\n",
        "        rated = [x for x in user_interactions if x['poem_id'] in self.id_to_idx]\n",
        "        if not rated:\n",
        "            return []\n",
        "        docs = [self.items[self.id_to_idx[x['poem_id']]]['content'] for x in rated]\n",
        "        vecs = self.vectorizer.transform(docs).toarray()\n",
        "        ratings = np.array([x.get('rating', 3.0) for x in rated])\n",
        "        weights = np.clip(ratings - 2.5, 0, None)\n",
        "        profile = np.average(vecs, axis=0, weights=weights) if weights.sum() > 0 else np.mean(vecs, axis=0)\n",
        "        sims = cosine_similarity([profile], self.item_vectors)[0]\n",
        "        recs = []\n",
        "        for i, s in enumerate(sims):\n",
        "            pid = self.items[i]['id']\n",
        "            if pid in exclude_ids:\n",
        "                continue\n",
        "            recs.append({'poem_id': pid, 'score': float(s)})\n",
        "        recs.sort(key=lambda x: x['score'], reverse=True)\n",
        "        return recs[:top_k]\n",
        "\n",
        "class ItemBasedCF:\n",
        "    def __init__(self):\n",
        "        self.item_sim = None\n",
        "        self.item_ids = None\n",
        "        self.item_id_to_idx = {}\n",
        "        self.idx_to_item_id = {}\n",
        "        self.rating_matrix = None\n",
        "\n",
        "    def fit(self, interactions, item_ids):\n",
        "        users = sorted(set(x['user_id'] for x in interactions))\n",
        "        user_map = {u:i for i,u in enumerate(users)}\n",
        "        self.item_ids = list(item_ids)\n",
        "        self.item_id_to_idx = {pid:i for i,pid in enumerate(self.item_ids)}\n",
        "        self.idx_to_item_id = {i:pid for pid,i in self.item_id_to_idx.items()}\n",
        "\n",
        "        R = np.zeros((len(users), len(self.item_ids)))\n",
        "        for x in interactions:\n",
        "            u = user_map[x['user_id']]\n",
        "            p = self.item_id_to_idx.get(x['poem_id'])\n",
        "            if p is not None:\n",
        "                R[u, p] = x.get('rating', 3.0)\n",
        "        self.rating_matrix = R\n",
        "\n",
        "        n_items = R.shape[1]\n",
        "        sim = np.zeros((n_items, n_items))\n",
        "        for i in range(n_items):\n",
        "            sim[i, i] = 1.0\n",
        "            for j in range(i + 1, n_items):\n",
        "                mask = (R[:, i] > 0) & (R[:, j] > 0)\n",
        "                if mask.sum() == 0:\n",
        "                    s = 0.0\n",
        "                else:\n",
        "                    vi, vj = R[mask, i], R[mask, j]\n",
        "                    vi, vj = vi - vi.mean(), vj - vj.mean()\n",
        "                    s = float((vi*vj).sum() / (np.sqrt((vi**2).sum()) * np.sqrt((vj**2).sum()) + 1e-8))\n",
        "                sim[i, j] = s\n",
        "                sim[j, i] = s\n",
        "        self.item_sim = sim\n",
        "\n",
        "    def recommend(self, user_interactions, exclude_ids, top_k=10):\n",
        "        if self.item_sim is None:\n",
        "            return []\n",
        "        user_ratings = np.zeros(len(self.item_ids))\n",
        "        for x in user_interactions:\n",
        "            idx = self.item_id_to_idx.get(x['poem_id'])\n",
        "            if idx is not None:\n",
        "                user_ratings[idx] = x.get('rating', 3.0)\n",
        "\n",
        "        rated = np.where(user_ratings > 0)[0]\n",
        "        if len(rated) == 0:\n",
        "            return []\n",
        "\n",
        "        scores = np.zeros(len(self.item_ids))\n",
        "        for i in range(len(self.item_ids)):\n",
        "            if user_ratings[i] > 0:\n",
        "                continue\n",
        "            neigh = self.item_sim[i, rated]\n",
        "            rr = user_ratings[rated]\n",
        "            m = neigh > 0\n",
        "            if m.sum() > 0:\n",
        "                scores[i] = np.dot(neigh[m], rr[m]) / (np.abs(neigh[m]).sum() + 1e-8)\n",
        "\n",
        "        recs = []\n",
        "        for i, s in enumerate(scores):\n",
        "            pid = self.idx_to_item_id[i]\n",
        "            if pid not in exclude_ids:\n",
        "                recs.append({'poem_id': pid, 'score': float(s)})\n",
        "        recs.sort(key=lambda x: x['score'], reverse=True)\n",
        "        return recs[:top_k]\n",
        "\n",
        "class UserBasedCF:\n",
        "    def __init__(self, k_neighbors=40):\n",
        "        self.k_neighbors = k_neighbors\n",
        "        self.R = None\n",
        "        self.user_map = {}\n",
        "        self.item_map = {}\n",
        "        self.idx_to_item = {}\n",
        "        self.user_sim = None\n",
        "\n",
        "    def fit(self, interactions, item_ids):\n",
        "        users = sorted(set(x['user_id'] for x in interactions))\n",
        "        self.user_map = {u:i for i,u in enumerate(users)}\n",
        "        self.item_map = {pid:i for i,pid in enumerate(item_ids)}\n",
        "        self.idx_to_item = {i:pid for pid,i in self.item_map.items()}\n",
        "\n",
        "        R = np.zeros((len(users), len(item_ids)))\n",
        "        for x in interactions:\n",
        "            u = self.user_map[x['user_id']]\n",
        "            p = self.item_map.get(x['poem_id'])\n",
        "            if p is not None:\n",
        "                R[u,p] = x.get('rating', 3.0)\n",
        "        self.R = R\n",
        "\n",
        "        n = R.shape[0]\n",
        "        sim = np.zeros((n, n))\n",
        "        for i in range(n):\n",
        "            sim[i,i]=1.0\n",
        "            for j in range(i+1,n):\n",
        "                m = (R[i]>0)&(R[j]>0)\n",
        "                if m.sum()==0:\n",
        "                    s=0.0\n",
        "                else:\n",
        "                    vi, vj = R[i,m], R[j,m]\n",
        "                    vi, vj = vi-vi.mean(), vj-vj.mean()\n",
        "                    s=float((vi*vj).sum()/(np.sqrt((vi**2).sum())*np.sqrt((vj**2).sum())+1e-8))\n",
        "                sim[i,j]=sim[j,i]=s\n",
        "        self.user_sim = sim\n",
        "\n",
        "    def recommend(self, user_interactions, exclude_ids, top_k=10):\n",
        "        if not user_interactions:\n",
        "            return []\n",
        "        uid = user_interactions[0]['user_id']\n",
        "        tidx = self.user_map.get(uid)\n",
        "        if tidx is None:\n",
        "            return []\n",
        "\n",
        "        sims = self.user_sim[tidx].copy()\n",
        "        sims[tidx] = -np.inf\n",
        "        neigh_idx = np.argsort(sims)[-self.k_neighbors:]\n",
        "        neigh = [(i, sims[i]) for i in neigh_idx if sims[i] > 0]\n",
        "        if not neigh:\n",
        "            return []\n",
        "\n",
        "        recs = []\n",
        "        for i in range(self.R.shape[1]):\n",
        "            pid = self.idx_to_item[i]\n",
        "            if pid in exclude_ids:\n",
        "                continue\n",
        "            ws, ss = 0.0, 0.0\n",
        "            for nidx, nsim in neigh:\n",
        "                r = self.R[nidx, i]\n",
        "                if r > 0:\n",
        "                    ws += nsim * r\n",
        "                    ss += abs(nsim)\n",
        "            if ss > 0:\n",
        "                recs.append({'poem_id': pid, 'score': float(ws/(ss+1e-8))})\n",
        "\n",
        "        recs.sort(key=lambda x: x['score'], reverse=True)\n",
        "        return recs[:top_k]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bertopic-model"
      },
      "outputs": [],
      "source": [
        "# BERT-Enhanced：优先使用你仓库里的 BERTopicEnhancedCF 类\n",
        "# 如果在 Colab 没有该源码，可把 backend/core/bertopic_enhanced_cf.py 上传后再运行。\n",
        "\n",
        "BERTopicEnhancedCF = None\n",
        "try:\n",
        "    from backend.core.bertopic_enhanced_cf import BERTopicEnhancedCF\n",
        "except Exception as e:\n",
        "    print('暂时无法 import BERTopicEnhancedCF:', e)\n",
        "\n",
        "print('BERTopicEnhancedCF loaded =', BERTopicEnhancedCF is not None)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eval"
      },
      "outputs": [],
      "source": [
        "def evaluate_topn(method_name, model, user_train, user_test, ks=(5,10), positive_threshold=4.0, all_train=None):\n",
        "    metrics = {k: {'precision': [], 'recall': [], 'hit': []} for k in ks}\n",
        "    users = sorted(set(user_train.keys()) & set(user_test.keys()))\n",
        "\n",
        "    for uid in users:\n",
        "        train_inter = user_train[uid]\n",
        "        test_inter = user_test[uid]\n",
        "\n",
        "        relevant = {x['poem_id'] for x in test_inter if x['rating'] >= positive_threshold}\n",
        "        if not train_inter or not relevant:\n",
        "            continue\n",
        "\n",
        "        exclude = {x['poem_id'] for x in train_inter}\n",
        "        max_k = max(ks)\n",
        "\n",
        "        if method_name == 'BERT-Enhanced':\n",
        "            recs = model.recommend(train_inter, all_train or [], top_k=max_k)\n",
        "        else:\n",
        "            recs = model.recommend(train_inter, exclude_ids=exclude, top_k=max_k)\n",
        "\n",
        "        ranked = [x['poem_id'] for x in recs]\n",
        "        for k in ks:\n",
        "            topk = ranked[:k]\n",
        "            hits = len(set(topk) & relevant)\n",
        "            metrics[k]['precision'].append(hits / k)\n",
        "            metrics[k]['recall'].append(hits / len(relevant))\n",
        "            metrics[k]['hit'].append(1.0 if hits > 0 else 0.0)\n",
        "\n",
        "    out = {}\n",
        "    for k in ks:\n",
        "        out[f'Precision@{k}'] = float(np.mean(metrics[k]['precision'])) if metrics[k]['precision'] else 0.0\n",
        "        out[f'Recall@{k}'] = float(np.mean(metrics[k]['recall'])) if metrics[k]['recall'] else 0.0\n",
        "        out[f'Hit@{k}'] = float(np.mean(metrics[k]['hit'])) if metrics[k]['hit'] else 0.0\n",
        "    return out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "run"
      },
      "outputs": [],
      "source": [
        "item_ids = [m['id'] for m in movies]\n",
        "\n",
        "cb = ContentBasedRecommender()\n",
        "cb.fit(movies)\n",
        "\n",
        "item_cf = ItemBasedCF()\n",
        "item_cf.fit(train, item_ids)\n",
        "\n",
        "user_cf = UserBasedCF()\n",
        "user_cf.fit(train, item_ids)\n",
        "\n",
        "methods = {\n",
        "    'CB': cb,\n",
        "    'Item-CF': item_cf,\n",
        "    'User-CF': user_cf,\n",
        "}\n",
        "\n",
        "if BERTopicEnhancedCF is not None:\n",
        "    bert_enhanced = BERTopicEnhancedCF(item_cf_weight=0.5, user_cf_weight=0.3, topic_weight=0.2)\n",
        "    bert_enhanced.fit(movies, train)\n",
        "    methods['BERT-Enhanced'] = bert_enhanced\n",
        "else:\n",
        "    print('跳过 BERT-Enhanced（未成功导入类）')\n",
        "\n",
        "rows = []\n",
        "for name, model in methods.items():\n",
        "    m = evaluate_topn(name, model, user_train, user_test, ks=cfg.top_ks, positive_threshold=cfg.positive_threshold, all_train=train)\n",
        "    rows.append({'method': name, **m})\n",
        "\n",
        "df_results = pd.DataFrame(rows).sort_values('Precision@10', ascending=False).reset_index(drop=True)\n",
        "df_results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "plot"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(8, 4.5))\n",
        "bars = plt.bar(df_results['method'], df_results['Precision@10'])\n",
        "plt.title('MovieLens-100K Precision@10 Comparison')\n",
        "plt.ylabel('Precision@10')\n",
        "for b, v in zip(bars, df_results['Precision@10']):\n",
        "    plt.text(b.get_x() + b.get_width()/2, b.get_height(), f'{v:.3f}', ha='center', va='bottom')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "save"
      },
      "outputs": [],
      "source": [
        "os.makedirs('./outputs', exist_ok=True)\n",
        "csv_path = './outputs/movielens_results_colab.csv'\n",
        "json_path = './outputs/movielens_results_colab.json'\n",
        "png_path = './outputs/movielens_precision_colab.png'\n",
        "\n",
        "df_results.to_csv(csv_path, index=False, encoding='utf-8')\n",
        "\n",
        "payload = {\n",
        "    'config': cfg.__dict__,\n",
        "    'dataset_stats': {\n",
        "        'n_users': len(set(x['user_id'] for x in ratings)),\n",
        "        'n_items': len(movies),\n",
        "        'n_ratings': len(ratings),\n",
        "        'n_train': len(train),\n",
        "        'n_test': len(test),\n",
        "    },\n",
        "    'results': df_results.to_dict(orient='records')\n",
        "}\n",
        "with open(json_path, 'w', encoding='utf-8') as f:\n",
        "    json.dump(payload, f, ensure_ascii=False, indent=2)\n",
        "\n",
        "plt.figure(figsize=(8, 4.5))\n",
        "bars = plt.bar(df_results['method'], df_results['Precision@10'])\n",
        "plt.title('MovieLens-100K Precision@10 Comparison')\n",
        "plt.ylabel('Precision@10')\n",
        "for b, v in zip(bars, df_results['Precision@10']):\n",
        "    plt.text(b.get_x() + b.get_width()/2, b.get_height(), f'{v:.3f}', ha='center', va='bottom')\n",
        "plt.tight_layout()\n",
        "plt.savefig(png_path, dpi=150)\n",
        "plt.close()\n",
        "\n",
        "print('Saved:', csv_path, json_path, png_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "notes"
      },
      "source": [
        "## Colab 使用说明\n",
        "\n",
        "1. 直接顺序运行全部单元即可。\n",
        "2. 若你要启用 `BERT-Enhanced`：请确保当前 runtime 可导入 `backend.core.bertopic_enhanced_cf`。\n",
        "3. 如果你不想安装 BERTopic 依赖，也可以先只跑 3 个 baseline。\n",
        "4. 结果文件在 `./outputs/` 下。"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}