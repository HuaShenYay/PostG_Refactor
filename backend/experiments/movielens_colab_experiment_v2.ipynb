{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# MovieLens-100K 实验（Colab独立版，算法对齐系统实现）\n",
        "\n",
        "这个 notebook **完全不依赖仓库文件导入**，可单独上传到 Colab 运行。  \n",
        "同时实现与系统 `BERTopicEnhancedCF` 一致的核心逻辑：\n",
        "- Item-CF：Pearson（共同评分项）\n",
        "- User-CF：Pearson（共同评分项）\n",
        "- Hybrid User Similarity：`alpha * rating_sim + (1-alpha) * topic_sim`\n",
        "- Confidence Filter：最小相似度阈值 + 共同评分 shrinkage\n",
        "- Enhanced Item Similarity：`item_cf_weight * norm(item_sim) + topic_weight * norm(topic_sim)`\n",
        "\n",
        "对比方法：CB、Item-CF、User-CF、BERT-Enhanced。\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 1) 安装依赖（Colab）\n",
        "!pip -q install numpy pandas scikit-learn matplotlib tqdm sentence-transformers\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 2) 导入依赖\n",
        "import os\n",
        "import json\n",
        "import random\n",
        "import zipfile\n",
        "import urllib.request\n",
        "from dataclasses import dataclass\n",
        "from datetime import datetime\n",
        "from collections import defaultdict, Counter\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from sentence_transformers import SentenceTransformer\n",
        "\n",
        "print('imports ok')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 3) 配置\n",
        "@dataclass\n",
        "class Config:\n",
        "    dataset_dir: str = './ml-100k'\n",
        "    test_ratio: float = 0.2\n",
        "    seed: int = 42\n",
        "    positive_threshold: float = 4.0\n",
        "    top_ks: tuple = (5, 10)\n",
        "\n",
        "cfg = Config()\n",
        "cfg\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 4) 下载并加载 MovieLens-100K\n",
        "\n",
        "def download_movielens(dataset_dir='./ml-100k'):\n",
        "    if os.path.exists(dataset_dir):\n",
        "        print('dataset exists:', dataset_dir)\n",
        "        return\n",
        "    url = 'https://files.grouplens.org/datasets/movielens/ml-100k.zip'\n",
        "    zip_path = './ml-100k.zip'\n",
        "    print('downloading:', url)\n",
        "    urllib.request.urlretrieve(url, zip_path)\n",
        "    with zipfile.ZipFile(zip_path, 'r') as zf:\n",
        "        zf.extractall('./')\n",
        "    os.remove(zip_path)\n",
        "    print('download done')\n",
        "\n",
        "\n",
        "def load_movies(dataset_dir):\n",
        "    genre_names = [\n",
        "        'unknown','Action','Adventure','Animation','Children','Comedy','Crime',\n",
        "        'Documentary','Drama','Fantasy','Film-Noir','Horror','Musical','Mystery',\n",
        "        'Romance','Sci-Fi','Thriller','War','Western'\n",
        "    ]\n",
        "    movies=[]\n",
        "    with open(os.path.join(dataset_dir,'u.item'),'r',encoding='latin-1') as f:\n",
        "        for line in f:\n",
        "            p=line.strip().split('|')\n",
        "            mid=int(p[0])\n",
        "            title=p[1]\n",
        "            flags=p[5:24]\n",
        "            genres=[g for g,v in zip(genre_names,flags) if v=='1']\n",
        "            content=f\"{title} {' '.join(genres)} movie film\"\n",
        "            movies.append({'id':mid,'title':title,'genres':genres,'content':content})\n",
        "    return movies\n",
        "\n",
        "\n",
        "def load_ratings(dataset_dir):\n",
        "    out=[]\n",
        "    with open(os.path.join(dataset_dir,'u.data'),'r',encoding='utf-8') as f:\n",
        "        for line in f:\n",
        "            uid,mid,r,ts=line.strip().split('\t')\n",
        "            out.append({\n",
        "                'user_id': int(uid),\n",
        "                'poem_id': int(mid),\n",
        "                'rating': float(r),\n",
        "                'created_at': datetime.fromtimestamp(int(ts))\n",
        "            })\n",
        "    return out\n",
        "\n",
        "\n",
        "def split_by_user_random(interactions, test_ratio=0.2, seed=42):\n",
        "    rng = random.Random(seed)\n",
        "    by_user = defaultdict(list)\n",
        "    for x in interactions:\n",
        "        by_user[x['user_id']].append(x)\n",
        "\n",
        "    train,test=[],[]\n",
        "    for uid, xs in by_user.items():\n",
        "        xs=xs.copy(); rng.shuffle(xs)\n",
        "        n_test=max(1,int(len(xs)*test_ratio))\n",
        "        test.extend(xs[:n_test])\n",
        "        train.extend(xs[n_test:])\n",
        "    return train,test\n",
        "\n",
        "\n",
        "def build_user_index(interactions):\n",
        "    d=defaultdict(list)\n",
        "    for x in interactions:\n",
        "        d[x['user_id']].append(x)\n",
        "    return d\n",
        "\n",
        "\n",
        "download_movielens(cfg.dataset_dir)\n",
        "movies=load_movies(cfg.dataset_dir)\n",
        "ratings=load_ratings(cfg.dataset_dir)\n",
        "train,test=split_by_user_random(ratings,cfg.test_ratio,cfg.seed)\n",
        "user_train=build_user_index(train)\n",
        "user_test=build_user_index(test)\n",
        "\n",
        "print('movies:',len(movies),'ratings:',len(ratings),'users:',len(set(x['user_id'] for x in ratings)))\n",
        "print('train:',len(train),'test:',len(test))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 5) 三个baseline：CB / Item-CF / User-CF\n",
        "class ContentBasedRecommender:\n",
        "    def __init__(self):\n",
        "        self.vectorizer = TfidfVectorizer(stop_words='english', ngram_range=(1,2), max_features=8000)\n",
        "        self.items=None\n",
        "        self.item_vectors=None\n",
        "        self.id_to_idx={}\n",
        "\n",
        "    def fit(self, items):\n",
        "        self.items=items\n",
        "        self.id_to_idx={x['id']:i for i,x in enumerate(items)}\n",
        "        self.item_vectors=self.vectorizer.fit_transform([x.get('content','') for x in items])\n",
        "\n",
        "    def recommend(self, user_interactions, exclude_ids=None, top_k=10):\n",
        "        if self.item_vectors is None:\n",
        "            return []\n",
        "        exclude_ids=exclude_ids or set()\n",
        "        rated=[x for x in user_interactions if x['poem_id'] in self.id_to_idx]\n",
        "        if not rated:\n",
        "            return []\n",
        "        docs=[self.items[self.id_to_idx[x['poem_id']]]['content'] for x in rated]\n",
        "        vecs=self.vectorizer.transform(docs).toarray()\n",
        "        ratings=np.array([x.get('rating',3.0) for x in rated],dtype=float)\n",
        "        weights=np.clip(ratings-2.5,0,None)\n",
        "        profile=np.average(vecs,axis=0,weights=weights) if weights.sum()>0 else np.mean(vecs,axis=0)\n",
        "        sims=cosine_similarity([profile],self.item_vectors)[0]\n",
        "        recs=[]\n",
        "        for i,s in enumerate(sims):\n",
        "            pid=self.items[i]['id']\n",
        "            if pid in exclude_ids:\n",
        "                continue\n",
        "            recs.append({'poem_id':pid,'score':float(s)})\n",
        "        recs.sort(key=lambda x:x['score'], reverse=True)\n",
        "        return recs[:top_k]\n",
        "\n",
        "\n",
        "class ItemBasedCFRecommender:\n",
        "    def __init__(self):\n",
        "        self.item_similarity=None\n",
        "        self.rating_matrix=None\n",
        "        self.poem_id_to_idx={}\n",
        "        self.idx_to_poem_id={}\n",
        "\n",
        "    def fit(self, interactions, poem_ids):\n",
        "        self.poem_id_to_idx={pid:i for i,pid in enumerate(poem_ids)}\n",
        "        self.idx_to_poem_id={i:pid for pid,i in self.poem_id_to_idx.items()}\n",
        "        users=sorted(set(i['user_id'] for i in interactions))\n",
        "        user_id_to_idx={uid:i for i,uid in enumerate(users)}\n",
        "        R=np.zeros((len(users),len(poem_ids)))\n",
        "        for inter in interactions:\n",
        "            u=user_id_to_idx[inter['user_id']]\n",
        "            p=self.poem_id_to_idx.get(inter['poem_id'])\n",
        "            if p is not None:\n",
        "                R[u,p]=inter.get('rating',3.0)\n",
        "        self.rating_matrix=R\n",
        "        self._compute_similarity()\n",
        "\n",
        "    def _compute_similarity(self):\n",
        "        n_items=self.rating_matrix.shape[1]\n",
        "        sim=np.zeros((n_items,n_items))\n",
        "        for i in range(n_items):\n",
        "            sim[i,i]=1.0\n",
        "            for j in range(i+1,n_items):\n",
        "                mask=(self.rating_matrix[:,i]>0) & (self.rating_matrix[:,j]>0)\n",
        "                if mask.sum()==0:\n",
        "                    s=0.0\n",
        "                else:\n",
        "                    vi=self.rating_matrix[mask,i]; vj=self.rating_matrix[mask,j]\n",
        "                    vi=vi-vi.mean(); vj=vj-vj.mean()\n",
        "                    s=float((vi*vj).sum()/(np.sqrt((vi**2).sum())*np.sqrt((vj**2).sum())+1e-8))\n",
        "                sim[i,j]=sim[j,i]=s\n",
        "        self.item_similarity=sim\n",
        "\n",
        "    def recommend(self, user_interactions, exclude_ids=None, top_k=10):\n",
        "        exclude_ids=exclude_ids or set()\n",
        "        user_ratings=np.zeros(len(self.poem_id_to_idx))\n",
        "        for inter in user_interactions:\n",
        "            p=self.poem_id_to_idx.get(inter['poem_id'])\n",
        "            if p is not None:\n",
        "                user_ratings[p]=inter.get('rating',3.0)\n",
        "        rated=np.where(user_ratings>0)[0]\n",
        "        if len(rated)==0:\n",
        "            return []\n",
        "        scores=np.zeros(len(self.poem_id_to_idx))\n",
        "        for i in range(len(self.poem_id_to_idx)):\n",
        "            if user_ratings[i]>0:\n",
        "                continue\n",
        "            neighbors=self.item_similarity[i,rated]\n",
        "            rr=user_ratings[rated]\n",
        "            m=neighbors>0\n",
        "            if m.sum()>0:\n",
        "                scores[i]=np.dot(neighbors[m],rr[m])/(np.abs(neighbors[m]).sum()+1e-8)\n",
        "        recs=[]\n",
        "        for i,s in enumerate(scores):\n",
        "            pid=self.idx_to_poem_id[i]\n",
        "            if pid not in exclude_ids:\n",
        "                recs.append({'poem_id':pid,'score':float(s)})\n",
        "        recs.sort(key=lambda x:x['score'], reverse=True)\n",
        "        return recs[:top_k]\n",
        "\n",
        "\n",
        "class UserBasedCFRecommender:\n",
        "    def __init__(self, k_neighbors=40):\n",
        "        self.k_neighbors=k_neighbors\n",
        "        self.rating_matrix=None\n",
        "        self.user_id_to_idx={}\n",
        "        self.item_id_to_idx={}\n",
        "        self.idx_to_item_id={}\n",
        "        self.user_similarity=None\n",
        "\n",
        "    def fit(self, interactions, item_ids):\n",
        "        users=sorted(set(x['user_id'] for x in interactions))\n",
        "        self.user_id_to_idx={u:i for i,u in enumerate(users)}\n",
        "        self.item_id_to_idx={pid:i for i,pid in enumerate(item_ids)}\n",
        "        self.idx_to_item_id={i:pid for pid,i in self.item_id_to_idx.items()}\n",
        "        R=np.zeros((len(users),len(item_ids)))\n",
        "        for x in interactions:\n",
        "            u=self.user_id_to_idx[x['user_id']]\n",
        "            p=self.item_id_to_idx.get(x['poem_id'])\n",
        "            if p is not None:\n",
        "                R[u,p]=x.get('rating',3.0)\n",
        "        self.rating_matrix=R\n",
        "        self._compute_user_similarity()\n",
        "\n",
        "    def _compute_user_similarity(self):\n",
        "        n_users=self.rating_matrix.shape[0]\n",
        "        sim=np.zeros((n_users,n_users))\n",
        "        for i in range(n_users):\n",
        "            sim[i,i]=1.0\n",
        "            for j in range(i+1,n_users):\n",
        "                m=(self.rating_matrix[i]>0) & (self.rating_matrix[j]>0)\n",
        "                if m.sum()==0:\n",
        "                    s=0.0\n",
        "                else:\n",
        "                    vi=self.rating_matrix[i,m]; vj=self.rating_matrix[j,m]\n",
        "                    vi=vi-vi.mean(); vj=vj-vj.mean()\n",
        "                    s=float((vi*vj).sum()/(np.sqrt((vi**2).sum())*np.sqrt((vj**2).sum())+1e-8))\n",
        "                sim[i,j]=sim[j,i]=s\n",
        "        self.user_similarity=sim\n",
        "\n",
        "    def recommend(self, user_interactions, exclude_ids=None, top_k=10):\n",
        "        exclude_ids=exclude_ids or set()\n",
        "        if not user_interactions:\n",
        "            return []\n",
        "        uid=user_interactions[0]['user_id']\n",
        "        tidx=self.user_id_to_idx.get(uid)\n",
        "        if tidx is None:\n",
        "            return []\n",
        "        sims=self.user_similarity[tidx].copy()\n",
        "        sims[tidx]=-np.inf\n",
        "        neigh_idx=np.argsort(sims)[-self.k_neighbors:]\n",
        "        neigh=[(i,sims[i]) for i in neigh_idx if sims[i]>0]\n",
        "        if not neigh:\n",
        "            return []\n",
        "\n",
        "        recs=[]\n",
        "        for i in range(self.rating_matrix.shape[1]):\n",
        "            pid=self.idx_to_item_id[i]\n",
        "            if pid in exclude_ids:\n",
        "                continue\n",
        "            ws,ss=0.0,0.0\n",
        "            for nidx,nsim in neigh:\n",
        "                r=self.rating_matrix[nidx,i]\n",
        "                if r>0:\n",
        "                    ws+=nsim*r\n",
        "                    ss+=abs(nsim)\n",
        "            if ss>0:\n",
        "                recs.append({'poem_id':pid,'score':float(ws/(ss+1e-8))})\n",
        "        recs.sort(key=lambda x:x['score'], reverse=True)\n",
        "        return recs[:top_k]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 6) BERT-Enhanced（独立实现，逻辑对齐系统版）\n",
        "class BERTopicVectorizer:\n",
        "    \"\"\"在Colab独立生成主题/语义向量（SentenceTransformer embeddings）。\"\"\"\n",
        "    def __init__(self, model_name='paraphrase-multilingual-MiniLM-L12-v2'):\n",
        "        self.model_name=model_name\n",
        "        self.encoder=None\n",
        "        self.topic_matrix=None\n",
        "        self.poem_ids=[]\n",
        "\n",
        "    def fit(self, poems):\n",
        "        self.poem_ids=[p['id'] for p in poems]\n",
        "        docs=[p.get('content','') for p in poems]\n",
        "        self.encoder=SentenceTransformer(self.model_name)\n",
        "        self.topic_matrix=self.encoder.encode(docs, show_progress_bar=True)\n",
        "\n",
        "\n",
        "class BERTopicEnhancedCFStandalone:\n",
        "    \"\"\"与系统 BERTopicEnhancedCF 保持同构思路（便于Colab独立运行）。\"\"\"\n",
        "    def __init__(self, item_cf_weight=0.5, user_cf_weight=0.3, topic_weight=0.2):\n",
        "        self.item_cf_weight=item_cf_weight\n",
        "        self.user_cf_weight=user_cf_weight\n",
        "        self.topic_weight=topic_weight\n",
        "\n",
        "        self.poems=None\n",
        "        self.interactions=None\n",
        "        self.poem_ids=[]\n",
        "        self.poem_id_map={}\n",
        "\n",
        "        self.user_id_map={}\n",
        "        self.rating_matrix=None\n",
        "        self.item_similarity=None\n",
        "        self.user_similarity=None\n",
        "        self.hybrid_similarity=None\n",
        "        self.enhanced_similarity=None\n",
        "\n",
        "        self.k_neighbors=30\n",
        "        self.min_similarity=0.1\n",
        "        self.min_common_ratings=3\n",
        "        self.hybrid_alpha=0.6\n",
        "\n",
        "        self.bertopic=BERTopicVectorizer()\n",
        "\n",
        "    def fit(self, poems, interactions):\n",
        "        self.poems=poems\n",
        "        self.interactions=interactions\n",
        "        self.poem_ids=[p['id'] for p in poems]\n",
        "        self.poem_id_map={pid:i for i,pid in enumerate(self.poem_ids)}\n",
        "\n",
        "        self._build_rating_matrix(interactions)\n",
        "        self._compute_item_similarity()\n",
        "        self._compute_user_similarity()\n",
        "\n",
        "        # 与系统修复后的顺序一致：先得到topic_matrix，再算hybrid/enhanced\n",
        "        self.bertopic.fit(poems)\n",
        "        self._compute_hybrid_user_similarity()\n",
        "        self._compute_enhanced_similarity()\n",
        "\n",
        "    def _build_rating_matrix(self, interactions):\n",
        "        users=sorted(set(i['user_id'] for i in interactions))\n",
        "        self.user_id_map={uid:i for i,uid in enumerate(users)}\n",
        "        R=np.zeros((len(users), len(self.poem_ids)))\n",
        "        for inter in interactions:\n",
        "            u=self.user_id_map[inter['user_id']]\n",
        "            p=self.poem_id_map.get(inter['poem_id'])\n",
        "            if p is not None:\n",
        "                R[u,p]=inter.get('rating',3.0)\n",
        "        self.rating_matrix=R\n",
        "\n",
        "    def _compute_item_similarity(self):\n",
        "        n_items=self.rating_matrix.shape[1]\n",
        "        sim=np.zeros((n_items,n_items))\n",
        "        for i in range(n_items):\n",
        "            sim[i,i]=1.0\n",
        "            for j in range(i+1,n_items):\n",
        "                m=(self.rating_matrix[:,i]>0)&(self.rating_matrix[:,j]>0)\n",
        "                if m.sum()==0:\n",
        "                    s=0.0\n",
        "                else:\n",
        "                    vi=self.rating_matrix[m,i]; vj=self.rating_matrix[m,j]\n",
        "                    vi=vi-vi.mean(); vj=vj-vj.mean()\n",
        "                    s=float((vi*vj).sum()/(np.sqrt((vi**2).sum())*np.sqrt((vj**2).sum())+1e-8))\n",
        "                sim[i,j]=sim[j,i]=s\n",
        "        self.item_similarity=sim\n",
        "\n",
        "    def _compute_user_similarity(self):\n",
        "        n_users=self.rating_matrix.shape[0]\n",
        "        sim=np.zeros((n_users,n_users))\n",
        "        for i in range(n_users):\n",
        "            sim[i,i]=1.0\n",
        "            for j in range(i+1,n_users):\n",
        "                m=(self.rating_matrix[i]>0)&(self.rating_matrix[j]>0)\n",
        "                if m.sum()==0:\n",
        "                    s=0.0\n",
        "                else:\n",
        "                    vi=self.rating_matrix[i,m]; vj=self.rating_matrix[j,m]\n",
        "                    vi=vi-vi.mean(); vj=vj-vj.mean()\n",
        "                    s=float((vi*vj).sum()/(np.sqrt((vi**2).sum())*np.sqrt((vj**2).sum())+1e-8))\n",
        "                sim[i,j]=sim[j,i]=s\n",
        "        self.user_similarity=sim\n",
        "\n",
        "    @staticmethod\n",
        "    def _min_max_normalize(matrix):\n",
        "        mn,mx=matrix.min(),matrix.max()\n",
        "        if mx-mn<1e-8:\n",
        "            return np.zeros_like(matrix)\n",
        "        return (matrix-mn)/(mx-mn)\n",
        "\n",
        "    def _compute_hybrid_user_similarity(self):\n",
        "        rating_sim=self.user_similarity.copy()\n",
        "        if self.bertopic is not None and self.bertopic.topic_matrix is not None:\n",
        "            n_users=self.rating_matrix.shape[0]\n",
        "            topic_dim=self.bertopic.topic_matrix.shape[1]\n",
        "            user_topic=np.zeros((n_users,topic_dim))\n",
        "            for u in range(n_users):\n",
        "                rated=self.rating_matrix[u]>0\n",
        "                if rated.sum()>0:\n",
        "                    ratings=self.rating_matrix[u,rated]\n",
        "                    vecs=self.bertopic.topic_matrix[rated]\n",
        "                    w=ratings/(ratings.sum()+1e-8)\n",
        "                    user_topic[u]=np.sum(vecs*w[:,None],axis=0)\n",
        "            topic_sim=cosine_similarity(user_topic)\n",
        "            self.hybrid_similarity=self.hybrid_alpha*rating_sim+(1-self.hybrid_alpha)*topic_sim\n",
        "        else:\n",
        "            self.hybrid_similarity=rating_sim\n",
        "\n",
        "    def _compute_enhanced_similarity(self):\n",
        "        item_sim=self.item_similarity\n",
        "        n_items=len(self.poem_ids)\n",
        "        if self.bertopic is not None and self.bertopic.topic_matrix is not None:\n",
        "            topic_sim=cosine_similarity(self.bertopic.topic_matrix)\n",
        "        else:\n",
        "            topic_sim=np.zeros((n_items,n_items)); np.fill_diagonal(topic_sim,1.0)\n",
        "\n",
        "        if item_sim.shape!=topic_sim.shape:\n",
        "            m=min(item_sim.shape[0],topic_sim.shape[0])\n",
        "            item_sim=item_sim[:m,:m]; topic_sim=topic_sim[:m,:m]\n",
        "            n_items=m\n",
        "\n",
        "        item_norm=self._min_max_normalize(item_sim[:n_items,:n_items])\n",
        "        topic_norm=self._min_max_normalize(topic_sim[:n_items,:n_items])\n",
        "        self.enhanced_similarity=self.item_cf_weight*item_norm + self.topic_weight*topic_norm\n",
        "\n",
        "    def _apply_confidence_filter(self, sim_matrix):\n",
        "        n=sim_matrix.shape[0]\n",
        "        filtered=sim_matrix.copy()\n",
        "        for i in range(n):\n",
        "            for j in range(i+1,n):\n",
        "                common=((self.rating_matrix[i]>0)&(self.rating_matrix[j]>0)).sum()\n",
        "                if common < self.min_common_ratings:\n",
        "                    shrinkage=common/(common+10)\n",
        "                    filtered[i,j]*=shrinkage; filtered[j,i]*=shrinkage\n",
        "                if abs(filtered[i,j]) < self.min_similarity:\n",
        "                    filtered[i,j]=0; filtered[j,i]=0\n",
        "        return filtered\n",
        "\n",
        "    def _get_top_k_neighbors(self, target_idx, sim_matrix, k=None):\n",
        "        if k is None:\n",
        "            k=self.k_neighbors\n",
        "        sims=sim_matrix[target_idx].copy()\n",
        "        sims[target_idx]=-np.inf\n",
        "        idx=np.argsort(sims)[-k:]\n",
        "        vals=sims[idx]\n",
        "        m=vals>0\n",
        "        return list(zip(idx[m], vals[m]))\n",
        "\n",
        "    def _get_user_cf_scores(self, user_interactions, exclude_ids):\n",
        "        if not user_interactions or self.hybrid_similarity is None:\n",
        "            return {}\n",
        "        uid=user_interactions[0]['user_id']\n",
        "        target_idx=self.user_id_map.get(uid)\n",
        "        if target_idx is None:\n",
        "            return {}\n",
        "\n",
        "        target_rated={x['poem_id']:x.get('rating',3.0) for x in user_interactions if x['poem_id'] in self.poem_id_map}\n",
        "        if not target_rated:\n",
        "            return {}\n",
        "\n",
        "        filtered=self._apply_confidence_filter(self.hybrid_similarity)\n",
        "        neighbors=self._get_top_k_neighbors(target_idx, filtered)\n",
        "        if not neighbors:\n",
        "            return {}\n",
        "\n",
        "        scores={}\n",
        "        for item_idx,item_id in enumerate(self.poem_ids):\n",
        "            if item_id in exclude_ids or item_id in target_rated:\n",
        "                continue\n",
        "            ws,ss=0.0,0.0\n",
        "            for nidx,sim in neighbors:\n",
        "                nr=self.rating_matrix[nidx,item_idx]\n",
        "                if nr>0:\n",
        "                    neigh_rated=self.rating_matrix[nidx]>0\n",
        "                    if neigh_rated.sum()>0:\n",
        "                        neigh_mean=self.rating_matrix[nidx,neigh_rated].mean()\n",
        "                        ws += sim*(nr-neigh_mean)\n",
        "                        ss += abs(sim)\n",
        "            if ss>0:\n",
        "                target_mask=self.rating_matrix[target_idx]>0\n",
        "                if target_mask.sum()>0:\n",
        "                    target_mean=self.rating_matrix[target_idx,target_mask].mean()\n",
        "                    scores[item_id]=target_mean+ws/ss\n",
        "        return scores\n",
        "\n",
        "    def recommend(self, user_interactions, all_interactions=None, top_k=10):\n",
        "        if self.enhanced_similarity is None:\n",
        "            return self._popular_fallback(top_k)\n",
        "        exclude_ids=set(x['poem_id'] for x in user_interactions)\n",
        "\n",
        "        user_ratings=np.zeros(len(self.poem_ids))\n",
        "        for x in user_interactions:\n",
        "            idx=self.poem_id_map.get(x['poem_id'])\n",
        "            if idx is not None:\n",
        "                user_ratings[idx]=x.get('rating',3.0)\n",
        "\n",
        "        rated=np.where(user_ratings>0)[0]\n",
        "        if len(rated)==0:\n",
        "            return self._popular_fallback(top_k, exclude_ids)\n",
        "\n",
        "        item_scores=np.zeros(len(self.poem_ids))\n",
        "        for i in range(len(self.poem_ids)):\n",
        "            if user_ratings[i]>0:\n",
        "                continue\n",
        "            neigh=self.enhanced_similarity[i,rated]\n",
        "            rr=user_ratings[rated]\n",
        "            m=neigh>0\n",
        "            if m.sum()>0:\n",
        "                item_scores[i]=np.dot(neigh[m],rr[m])/(np.abs(neigh[m]).sum()+1e-8)\n",
        "\n",
        "        user_cf_scores=self._get_user_cf_scores(user_interactions, exclude_ids)\n",
        "\n",
        "        results=[]\n",
        "        for i,item_score in enumerate(item_scores):\n",
        "            pid=self.poem_ids[i]\n",
        "            if pid in exclude_ids:\n",
        "                continue\n",
        "            score=item_score\n",
        "            if pid in user_cf_scores and self.user_cf_weight>0:\n",
        "                max_ucf=max(user_cf_scores.values()) if user_cf_scores else 1\n",
        "                u_norm=user_cf_scores[pid]/max_ucf if max_ucf>0 else 0\n",
        "                score=(1-self.user_cf_weight)*score + self.user_cf_weight*u_norm\n",
        "            if score>0:\n",
        "                results.append({'poem_id':pid,'score':float(score)})\n",
        "        results.sort(key=lambda x:x['score'], reverse=True)\n",
        "        return results[:top_k]\n",
        "\n",
        "    def _popular_fallback(self, top_k, exclude_ids=None):\n",
        "        exclude_ids=exclude_ids or set()\n",
        "        c=Counter()\n",
        "        for x in self.interactions:\n",
        "            if x['poem_id'] not in exclude_ids:\n",
        "                c[x['poem_id']]+=x.get('rating',3.0)\n",
        "        return [{'poem_id':pid,'score':float(s)} for pid,s in c.most_common(top_k)]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 7) 训练四个方法\n",
        "item_ids=[m['id'] for m in movies]\n",
        "\n",
        "cb=ContentBasedRecommender(); cb.fit(movies)\n",
        "item_cf=ItemBasedCFRecommender(); item_cf.fit(train, item_ids)\n",
        "user_cf=UserBasedCFRecommender(); user_cf.fit(train, item_ids)\n",
        "bert_enhanced=BERTopicEnhancedCFStandalone(item_cf_weight=0.5, user_cf_weight=0.3, topic_weight=0.2)\n",
        "bert_enhanced.fit(movies, train)\n",
        "\n",
        "methods={\n",
        "    'CB': cb,\n",
        "    'Item-CF': item_cf,\n",
        "    'User-CF': user_cf,\n",
        "    'BERT-Enhanced': bert_enhanced,\n",
        "}\n",
        "print('models ready')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 8) Top-N 评估（P@K / R@K / Hit@K）\n",
        "def evaluate_topn(method_name, model, user_train, user_test, ks=(5,10), positive_threshold=4.0, all_train=None):\n",
        "    metrics={k:{'precision':[],'recall':[],'hit':[]} for k in ks}\n",
        "    users=sorted(set(user_train.keys()) & set(user_test.keys()))\n",
        "\n",
        "    for uid in users:\n",
        "        train_inter=user_train[uid]\n",
        "        test_inter=user_test[uid]\n",
        "        relevant={x['poem_id'] for x in test_inter if x.get('rating',0)>=positive_threshold}\n",
        "        if not train_inter or not relevant:\n",
        "            continue\n",
        "\n",
        "        exclude={x['poem_id'] for x in train_inter}\n",
        "        max_k=max(ks)\n",
        "        if method_name=='BERT-Enhanced':\n",
        "            recs=model.recommend(train_inter, all_train, top_k=max_k)\n",
        "        else:\n",
        "            recs=model.recommend(train_inter, exclude_ids=exclude, top_k=max_k)\n",
        "\n",
        "        ranked=[x['poem_id'] for x in recs]\n",
        "        for k in ks:\n",
        "            topk=ranked[:k]\n",
        "            hits=len(set(topk)&relevant)\n",
        "            metrics[k]['precision'].append(hits/k)\n",
        "            metrics[k]['recall'].append(hits/len(relevant))\n",
        "            metrics[k]['hit'].append(1.0 if hits>0 else 0.0)\n",
        "\n",
        "    out={}\n",
        "    for k in ks:\n",
        "        out[f'Precision@{k}']=float(np.mean(metrics[k]['precision'])) if metrics[k]['precision'] else 0.0\n",
        "        out[f'Recall@{k}']=float(np.mean(metrics[k]['recall'])) if metrics[k]['recall'] else 0.0\n",
        "        out[f'Hit@{k}']=float(np.mean(metrics[k]['hit'])) if metrics[k]['hit'] else 0.0\n",
        "    return out\n",
        "\n",
        "rows=[]\n",
        "for name,model in methods.items():\n",
        "    m=evaluate_topn(name, model, user_train, user_test, ks=cfg.top_ks, positive_threshold=cfg.positive_threshold, all_train=train)\n",
        "    rows.append({'method':name, **m})\n",
        "\n",
        "df_results=pd.DataFrame(rows).sort_values('Precision@10', ascending=False).reset_index(drop=True)\n",
        "df_results\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 9) 可视化 + 导出结果\n",
        "plt.figure(figsize=(8,4.5))\n",
        "bars=plt.bar(df_results['method'], df_results['Precision@10'])\n",
        "plt.title('MovieLens-100K Precision@10 Comparison')\n",
        "plt.ylabel('Precision@10')\n",
        "for b,v in zip(bars, df_results['Precision@10']):\n",
        "    plt.text(b.get_x()+b.get_width()/2, b.get_height(), f'{v:.3f}', ha='center', va='bottom')\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "os.makedirs('./outputs', exist_ok=True)\n",
        "csv_path='./outputs/movielens_results_colab.csv'\n",
        "json_path='./outputs/movielens_results_colab.json'\n",
        "png_path='./outputs/movielens_precision_colab.png'\n",
        "\n",
        "df_results.to_csv(csv_path, index=False, encoding='utf-8')\n",
        "\n",
        "payload={\n",
        "    'config': cfg.__dict__,\n",
        "    'dataset_stats': {\n",
        "        'n_users': len(set(x['user_id'] for x in ratings)),\n",
        "        'n_items': len(movies),\n",
        "        'n_ratings': len(ratings),\n",
        "        'n_train': len(train),\n",
        "        'n_test': len(test),\n",
        "    },\n",
        "    'results': df_results.to_dict(orient='records')\n",
        "}\n",
        "with open(json_path,'w',encoding='utf-8') as f:\n",
        "    json.dump(payload,f,ensure_ascii=False,indent=2)\n",
        "\n",
        "plt.figure(figsize=(8,4.5))\n",
        "bars=plt.bar(df_results['method'], df_results['Precision@10'])\n",
        "plt.title('MovieLens-100K Precision@10 Comparison')\n",
        "plt.ylabel('Precision@10')\n",
        "for b,v in zip(bars, df_results['Precision@10']):\n",
        "    plt.text(b.get_x()+b.get_width()/2, b.get_height(), f'{v:.3f}', ha='center', va='bottom')\n",
        "plt.tight_layout(); plt.savefig(png_path,dpi=150); plt.close()\n",
        "\n",
        "print('saved:', csv_path, json_path, png_path)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 说明\n",
        "- 该 notebook 不依赖 `backend/` 目录导入，可直接独立运行。  \n",
        "- 其中 `BERTopicEnhancedCFStandalone` 的关键流程和融合逻辑与系统实现保持一致。  \n",
        "- 若想进一步“逐行一致”，可把系统类定义直接复制到本 notebook 对应单元替换。\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}