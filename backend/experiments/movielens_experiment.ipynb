{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BERTopic-Enhanced-CF 推荐系统实验\n",
    "\n",
    "使用MovieLens-100k数据集验证BERTopic增强的协同过滤算法\n",
    "\n",
    "## 实验对比组\n",
    "- **Content-Based (CB)**: TF-IDF内容推荐\n",
    "- **Item-CF**: 传统协同过滤（评分矩阵）\n",
    "- **BERTopic-Enhanced-CF**: 0.6×评分相似度 + 0.4×主题相似度（核心创新）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 安装依赖 (GPU版本)\n",
    "!pip install numpy pandas scikit-learn bertopic sentence-transformers jieba torch --index-url https://download.pytorch.org/whl/cu121"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import json\n",
    "import random\n",
    "import urllib.request\n",
    "import zipfile\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "from datetime import datetime\n",
    "from typing import Dict, List, Tuple\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==================== 数据加载 ====================\n",
    "\n",
    "def download_movielens():\n",
    "    \"\"\"下载MovieLens-100k数据集\"\"\"\n",
    "    dataset_path = \"ml-100k\"\n",
    "    if os.path.exists(dataset_path):\n",
    "        print(\"数据集已存在\")\n",
    "        return dataset_path\n",
    "    \n",
    "    url = \"http://files.grouplens.org/datasets/movielens/ml-100k.zip\"\n",
    "    zip_path = \"ml-100k.zip\"\n",
    "    \n",
    "    print(\"下载MovieLens-100k数据集...\")\n",
    "    urllib.request.urlretrieve(url, zip_path)\n",
    "    \n",
    "    print(\"解压数据集...\")\n",
    "    with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
    "        zip_ref.extractall(\".\")\n",
    "    \n",
    "    os.remove(zip_path)\n",
    "    print(\"数据集准备完成\")\n",
    "    return dataset_path\n",
    "\n",
    "def load_movies(dataset_path: str) -> List[dict]:\n",
    "    \"\"\"加载电影数据\"\"\"\n",
    "    movies = {}\n",
    "    \n",
    "    genres_map = {\n",
    "        \"Action\": 1, \"Adventure\": 2, \"Animation\": 3, \"Children\": 4, \n",
    "        \"Comedy\": 5, \"Crime\": 6, \"Documentary\": 7, \"Drama\": 8, \n",
    "        \"Fantasy\": 9, \"FilmNoir\": 10, \"Horror\": 11, \"Musical\": 12, \n",
    "        \"Mystery\": 13, \"Romance\": 14, \"SciFi\": 15, \"Thriller\": 16, \n",
    "        \"War\": 17, \"Western\": 18\n",
    "    }\n",
    "    \n",
    "    item_path = os.path.join(dataset_path, \"u.item\")\n",
    "    with open(item_path, \"r\", encoding=\"latin-1\") as f:\n",
    "        for line in f:\n",
    "            parts = line.strip().split(\"|\")\n",
    "            mid = int(parts[0])\n",
    "            title = parts[1]\n",
    "            genres = parts[5:23]\n",
    "            genre_text = \" \".join([g for g, v in zip(genres_map.keys(), genres) if v == \"1\"])\n",
    "            \n",
    "            content = f\"{title} {genre_text} movie film {title.lower()}\"\n",
    "            \n",
    "            movies[mid] = {\n",
    "                \"id\": mid,\n",
    "                \"title\": title,\n",
    "                \"content\": content,\n",
    "                \"genres\": [g for g, v in zip(genres_map.keys(), genres) if v == \"1\"]\n",
    "            }\n",
    "    \n",
    "    print(f\"加载电影数量: {len(movies)}\")\n",
    "    return list(movies.values())\n",
    "\n",
    "def load_ratings(dataset_path: str) -> List[dict]:\n",
    "    \"\"\"加载评分数据\"\"\"\n",
    "    ratings = []\n",
    "    rating_path = os.path.join(dataset_path, \"u.data\")\n",
    "    \n",
    "    with open(rating_path, \"r\") as f:\n",
    "        for line in f:\n",
    "            parts = line.strip().split()\n",
    "            uid = int(parts[0])\n",
    "            mid = int(parts[1])\n",
    "            rating = float(parts[2])\n",
    "            timestamp = int(parts[3])\n",
    "            \n",
    "            ratings.append({\n",
    "                \"user_id\": uid,\n",
    "                \"poem_id\": mid,\n",
    "                \"rating\": rating,\n",
    "                \"liked\": rating >= 3.5,\n",
    "                \"created_at\": datetime.fromtimestamp(timestamp),\n",
    "            })\n",
    "    \n",
    "    print(f\"加载评分数量: {len(ratings)}\")\n",
    "    return ratings\n",
    "\n",
    "# 下载并加载数据\n",
    "dataset_path = download_movielens()\n",
    "movies = load_movies(dataset_path)\n",
    "ratings = load_ratings(dataset_path)\n",
    "print(f\"\\n数据集统计: {len(movies)}部电影, {len(ratings)}条评分\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==================== 评估指标 ====================\n",
    "\n",
    "def temporal_split(interactions: List[dict], test_ratio: float) -> Tuple[List[dict], List[dict]]:\n",
    "    \"\"\"时序划分\"\"\"\n",
    "    by_user = defaultdict(list)\n",
    "    for x in interactions:\n",
    "        by_user[x[\"user_id\"]].append(x)\n",
    "    \n",
    "    train, test = [], []\n",
    "    for _, xs in by_user.items():\n",
    "        xs.sort(key=lambda v: v[\"created_at\"])\n",
    "        n = len(xs)\n",
    "        n_test = max(1, int(n * test_ratio))\n",
    "        train.extend(xs[: n - n_test])\n",
    "        test.extend(xs[n - n_test:])\n",
    "    return train, test\n",
    "\n",
    "def dcg_at_k(rels: List[int], k: int) -> float:\n",
    "    rels = np.array(rels[:k])\n",
    "    if len(rels) == 0:\n",
    "        return 0.0\n",
    "    return float(np.sum((2**rels - 1) / np.log2(np.arange(2, len(rels) + 2))))\n",
    "\n",
    "def ndcg_at_k(recommended: List[int], relevant: set, k: int) -> float:\n",
    "    if not relevant:\n",
    "        return 0.0\n",
    "    rels = [1 if pid in relevant else 0 for pid in recommended[:k]]\n",
    "    idcg = dcg_at_k(sorted(rels, reverse=True), k)\n",
    "    return dcg_at_k(rels, k) / idcg if idcg > 0 else 0.0\n",
    "\n",
    "def mrr_at_k(recommended: List[int], relevant: set, k: int) -> float:\n",
    "    for idx, pid in enumerate(recommended[:k], start=1):\n",
    "        if pid in relevant:\n",
    "            return 1.0 / idx\n",
    "    return 0.0\n",
    "\n",
    "print(\"评估指标函数定义完成\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==================== 推荐算法实现 ====================\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "class EnglishCB:\n",
    "    \"\"\"英文内容推荐\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.vectorizer = TfidfVectorizer(max_features=1000, stop_words='english', ngram_range=(1,2))\n",
    "        self.tfidf_matrix = None\n",
    "        self.items = None\n",
    "        \n",
    "    def fit(self, items):\n",
    "        self.items = items\n",
    "        contents = [item.get(\"content\", \"\") for item in items]\n",
    "        self.tfidf_matrix = self.vectorizer.fit_transform(contents)\n",
    "        \n",
    "    def get_user_profile(self, rated_items, ratings):\n",
    "        if not rated_items:\n",
    "            return None\n",
    "        rated_contents = [item.get(\"content\", \"\") for item in rated_items]\n",
    "        rated_vectors = self.vectorizer.transform(rated_contents)\n",
    "        ratings_arr = np.array(ratings)\n",
    "        weights = np.abs((ratings_arr - 3.0) / 2.0)\n",
    "        if weights.sum() > 0:\n",
    "            user_profile = np.average(rated_vectors.toarray(), axis=0, weights=weights)\n",
    "        else:\n",
    "            user_profile = np.mean(rated_vectors.toarray(), axis=0)\n",
    "        return user_profile\n",
    "        \n",
    "    def recommend(self, user_profile, exclude_ids, top_k):\n",
    "        if user_profile is None or self.tfidf_matrix is None:\n",
    "            return []\n",
    "        exclude_ids = exclude_ids or set()\n",
    "        similarities = cosine_similarity([user_profile], self.tfidf_matrix.toarray())[0]\n",
    "        results = []\n",
    "        for i, item in enumerate(self.items):\n",
    "            if item[\"id\"] not in exclude_ids:\n",
    "                results.append({\"poem_id\": item[\"id\"], \"score\": float(similarities[i])})\n",
    "        results.sort(key=lambda x: x[\"score\"], reverse=True)\n",
    "        return results[:top_k]\n",
    "\n",
    "\n",
    "class ItemBasedCFRecommender:\n",
    "    \"\"\"传统Item-CF\"\"\"\n",
    "    \n",
    "    def __init__(self, k_neighbors=30):\n",
    "        self.k_neighbors = k_neighbors\n",
    "        self.item_similarity = None\n",
    "        self.rating_matrix = None\n",
    "        self.poem_id_to_idx = None\n",
    "        self.idx_to_poem_id = None\n",
    "        \n",
    "    def fit(self, interactions, poem_ids):\n",
    "        self.poem_id_to_idx = {pid: idx for idx, pid in enumerate(poem_ids)}\n",
    "        self.idx_to_poem_id = {idx: pid for pid, idx in self.poem_id_to_idx.items()}\n",
    "\n",
    "        users = set(i[\"user_id\"] for i in interactions)\n",
    "        user_id_to_idx = {uid: idx for idx, uid in enumerate(users)}\n",
    "\n",
    "        n_users = len(users)\n",
    "        n_items = len(poem_ids)\n",
    "\n",
    "        self.rating_matrix = np.zeros((n_users, n_items))\n",
    "\n",
    "        for inter in interactions:\n",
    "            u_idx = user_id_to_idx[inter[\"user_id\"]]\n",
    "            p_idx = self.poem_id_to_idx[inter[\"poem_id\"]]\n",
    "            self.rating_matrix[u_idx, p_idx] = inter.get(\"rating\", 3.0)\n",
    "\n",
    "        self._compute_similarity()\n",
    "        print(f\"[Item-CF] 评分矩阵: {self.rating_matrix.shape}\")\n",
    "        \n",
    "    def _compute_similarity(self):\n",
    "        n_items = self.rating_matrix.shape[1]\n",
    "        self.item_similarity = np.zeros((n_items, n_items))\n",
    "\n",
    "        for i in range(n_items):\n",
    "            for j in range(i, n_items):\n",
    "                if i == j:\n",
    "                    self.item_similarity[i, j] = 1.0\n",
    "                    continue\n",
    "                \n",
    "                mask = (self.rating_matrix[:, i] > 0) & (self.rating_matrix[:, j] > 0)\n",
    "                if mask.sum() == 0:\n",
    "                    similarity = 0.0\n",
    "                else:\n",
    "                    vec_i = self.rating_matrix[mask, i]\n",
    "                    vec_j = self.rating_matrix[mask, j]\n",
    "                    mean_i = vec_i.mean()\n",
    "                    mean_j = vec_j.mean()\n",
    "\n",
    "                    if mean_i > 0 and mean_j > 0:\n",
    "                        sim = np.sum((vec_i - mean_i) * (vec_j - mean_j)) / (\n",
    "                            np.sqrt(np.sum((vec_i - mean_i) ** 2)) * np.sqrt(np.sum((vec_j - mean_j) ** 2)) + 1e-8\n",
    "                        )\n",
    "                        similarity = sim\n",
    "                    else:\n",
    "                        similarity = 0.0\n",
    "\n",
    "                self.item_similarity[i, j] = similarity\n",
    "                self.item_similarity[j, i] = similarity\n",
    "\n",
    "    def recommend(self, user_interactions, exclude_ids=None, top_k=10):\n",
    "        exclude_ids = exclude_ids or set()\n",
    "        \n",
    "        user_ratings = np.zeros(len(self.poem_id_to_idx))\n",
    "        for inter in user_interactions:\n",
    "            if inter[\"poem_id\"] in self.poem_id_to_idx:\n",
    "                p_idx = self.poem_id_to_idx[inter[\"poem_id\"]]\n",
    "                user_ratings[p_idx] = inter.get(\"rating\", 3.0)\n",
    "        \n",
    "        rated_items = np.where(user_ratings > 0)[0]\n",
    "        if len(rated_items) == 0:\n",
    "            return []\n",
    "        \n",
    "        scores = np.zeros(len(self.poem_id_to_idx))\n",
    "        \n",
    "        for item_idx in range(len(self.poem_id_to_idx)):\n",
    "            if user_ratings[item_idx] > 0:\n",
    "                continue\n",
    "            \n",
    "            neighbors = self.item_similarity[item_idx, rated_items]\n",
    "            neighbor_ratings = user_ratings[rated_items]\n",
    "            \n",
    "            pos_mask = neighbors > 0\n",
    "            if pos_mask.sum() > 0:\n",
    "                scores[item_idx] = np.dot(neighbors[pos_mask], neighbor_ratings[pos_mask]) / (np.abs(neighbors[pos_mask]).sum() + 1e-8)\n",
    "        \n",
    "        results = []\n",
    "        for idx, score in enumerate(scores):\n",
    "            poem_id = self.idx_to_poem_id[idx]\n",
    "            if poem_id not in exclude_ids and user_ratings[idx] == 0:\n",
    "                results.append({\"poem_id\": poem_id, \"score\": float(score)})\n",
    "        \n",
    "        results.sort(key=lambda x: x[\"score\"], reverse=True)\n",
    "        return results[:top_k]\n",
    "\n",
    "\n",
    "class BERTopicEnhancedCF:\n",
    "    \"\"\"BERTopic增强的协同过滤 - 核心创新\"\"\"\n",
    "    \n",
    "    def __init__(self, rating_weight=0.6, topic_weight=0.4):\n",
    "        self.rating_weight = rating_weight\n",
    "        self.topic_weight = topic_weight\n",
    "        self.item_cf = None\n",
    "        self.bertopic = None\n",
    "        self.items = None\n",
    "        self.interactions = None\n",
            "        self.item_ids = []\n",
    "        self.item_id_map = {}\n",
    "        self.enhanced_similarity = None\n",
    "        \n",
    "    def fit(self, items, interactions):\n",
    "        self.items = items\n",
    "        self.interactions = interactions\n",
    "        self.item_ids = [p[\"id\"] for p in items]\n",
    "        self.item_id_map = {pid: idx for idx, pid in enumerate(self.item_ids)}\n",
    "        \n",
    "        print(\"训练Item-CF模型...\")\n",
    "        self.item_cf = ItemBasedCFRecommender()\n",
    "        self.item_cf.fit(interactions, self.item_ids)\n",
    "        \n",
    "        print(\"训练BERTopic模型...\")\n",
    "        self._build_bertopic_model()\n",
    "        \n",
    "        print(\"计算增强相似度矩阵...\")\n",
    "        self._compute_enhanced_similarity()\n",
    "        print(\"模型训练完成!\")\n",
    "        \n",
    "    def _build_bertopic_model(self):\n",
    "        \"\"\"构建BERTopic模型 - GPU加速版本\"\"\"\n",
    "        try:\n",
    "            from sentence_transformers import SentenceTransformer\n",
    "            from bertopic import BERTopic\n",
    "            from sklearn.feature_extraction.text import CountVectorizer\n",
    "            \n",
    "            # 检测GPU并设置device\n",
    "            import torch\n",
    "            device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "            print(f\"使用设备: {device}\")\n",
    "            \n",
    "            print(\"加载BERTopic模型...\")\n",
    "            embedding_model = SentenceTransformer(\n",
    "                \"paraphrase-multilingual-MiniLM-L12-v2\", \n",
    "                device=device  # GPU加速\n",
    "            )\n",
    "            \n",
    "            contents = [item.get(\"content\", \"\") for item in self.items]\n",
    "            \n",
    "            try:\n",
    "                self.bertopic_model = BERTopic(embedding_model=embedding_model, nr_topics=\"auto\")\n",
    "                topics, probs = self.bertopic_model.fit_transform(contents)\n",
    "                # GPU加速编码\n",
    "                self.topic_matrix = embedding_model.encode(\n",
    "                    contents, \n",
    "                    show_progress_bar=True,\n",
    "                    batch_size=64  # 批量处理加速\n",
    "                )\n",
    "            except Exception as e:\n",
    "                print(f\"BERTopic聚类失败: {e}，使用原始embedding\")\n",
    "                self.topic_matrix = embedding_model.encode(\n",
    "                    contents, \n",
    "                    show_progress_bar=True,\n",
    "                    batch_size=64\n",
    "                )\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"BERTopic加载失败: {e}，使用TF-IDF作为fallback\")\n",
    "            vectorizer = TfidfVectorizer(max_features=384, stop_words='english')\n",
    "            contents = [item.get(\"content\", \"\") for item in self.items]\n",
    "            self.topic_matrix = vectorizer.fit_transform(contents).toarray()\n",
    "    \n",
    "    def _min_max_normalize(self, matrix):"\n",
    "        \"\"\"Min-Max归一化\"\"\"\n",
    "        min_val = matrix.min()\n",
    "        max_val = matrix.max()\n",
    "        if max_val - min_val < 1e-8:\n",
    "            return np.zeros_like(matrix)\n",
    "        return (matrix - min_val) / (max_val - min_val)\n",
    "    \n",
    "    def _compute_enhanced_similarity(self):\n",
    "        \"\"\"计算增强相似度矩阵\"\"\"\n",
    "        n_items = len(self.item_ids)\n",
    "        \n",
    "        rating_sim = self.item_cf.item_similarity\n",
    "        topic_sim = cosine_similarity(self.topic_matrix)\n",
    "        \n",
    "        if rating_sim.shape != topic_sim.shape:\n",
    "            min_dim = min(rating_sim.shape[0], topic_sim.shape[0])\n",
    "            rating_sim = rating_sim[:min_dim, :min_dim]\n",
    "            topic_sim = topic_sim[:min_dim, :min_dim]\n",
    "            n_items = min_dim\n",
    "        \n",
    "        rating_norm = self._min_max_normalize(rating_sim[:n_items, :n_items])\n",
    "        topic_norm = self._min_max_normalize(topic_sim[:n_items, :n_items])\n",
    "        \n",
    "        self.enhanced_similarity = (\n",
    "            self.rating_weight * rating_norm + \n",
    "            self.topic_weight * topic_norm\n",
    "        )\n",
    "        print(f\"融合完成: {self.rating_weight}×评分 + {self.topic_weight}×主题\")\n",
    "    \n",
    "    def recommend(self, user_interactions, all_interactions, top_k=10):\n",
    "        if self.enhanced_similarity is None:\n",
    "            return self._popular_fallback(top_k)\n",
    "        \n",
    "        exclude_ids = set(i[\"poem_id\"] for i in user_interactions)\n",
    "        \n",
    "        user_ratings = np.zeros(len(self.item_ids))\n",
    "        for inter in user_interactions:\n",
    "            if inter[\"poem_id\"] in self.item_id_map:\n",
    "                idx = self.item_id_map[inter[\"poem_id\"]]\n",
    "                user_ratings[idx] = inter.get(\"rating\", 3.0)\n",
    "        \n",
    "        rated_indices = np.where(user_ratings > 0)[0]\n",
    "        if len(rated_indices) == 0:\n",
    "            return self._popular_fallback(top_k, exclude_ids)\n",
    "        \n",
    "        scores = np.zeros(len(self.item_ids))\n",
    "        for i in range(len(self.item_ids)):\n",
    "            if user_ratings[i] > 0:\n",
    "                continue\n",
    "            \n",
    "            neighbors = self.enhanced_similarity[i, rated_indices]\n",
    "            neighbor_ratings = user_ratings[rated_indices]\n",
    "            \n",
    "            pos_mask = neighbors > 0\n",
    "            if pos_mask.sum() > 0:\n",
    "                scores[i] = np.dot(neighbors[pos_mask], neighbor_ratings[pos_mask]) / (np.abs(neighbors[pos_mask]).sum() + 1e-8)\n",
    "        \n",
    "        results = []\n",
    "        for idx, score in enumerate(scores):\n",
    "            item_id = self.item_ids[idx]\n",
    "            if item_id not in exclude_ids and score > 0:\n",
    "                results.append({\"poem_id\": item_id, \"score\": float(score)})\n",
    "        \n",
    "        results.sort(key=lambda x: x[\"score\"], reverse=True)\n",
    "        return results[:top_k]\n",
    "    \n",
    "    def _popular_fallback(self, top_k, exclude_ids=None):\n",
    "        exclude_ids = exclude_ids or set()\n",
    "        item_scores = defaultdict(float)\n",
    "        for inter in self.interactions:\n",
    "            if inter[\"poem_id\"] not in exclude_ids:\n",
    "                item_scores[inter[\"poem_id\"]] += inter.get(\"rating\", 3.0)\n",
    "        \n",
    "        sorted_items = sorted(item_scores.items(), key=lambda x: x[1], reverse=True)\n",
    "        return [{\"poem_id\": pid, \"score\": float(score)} for pid, score in sorted_items[:top_k]]\n",
    "\n",
    "\n",
    "print(\"推荐算法定义完成\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==================== 评估器 ====================\n",
    "\n",
    "class Evaluator:\n",
    "    def __init__(self, items, top_k, threshold):\n",
    "        self.items = items\n",
    "        self.top_k = top_k\n",
    "        self.threshold = threshold\n",
    "        self.pid_to_idx = {p[\"id\"]: idx for idx, p in enumerate(items)}\n",
    "\n",
    "    def evaluate(self, models, train_data, test_data):\n",
    "        user_train = defaultdict(list)\n",
    "        user_test = defaultdict(list)\n",
    "        for x in train_data:\n",
    "            user_train[x[\"user_id\"]].append(x)\n",
    "        for x in test_data:\n",
    "            user_test[x[\"user_id\"]].append(x)\n",
    "\n",
    "        results = {}\n",
    "\n",
    "        for name, info in models.items():\n",
    "            rec = info[\"recommender\"]\n",
    "            kind = info[\"type\"]\n",
    "            p_list, r_list, f1_list, ndcg_list, mrr_list = [], [], [], [], []\n",
    "            all_rec_items = set()\n",
    "\n",
    "            for inter in test_data:\n",
    "                uid, pid, actual = inter[\"user_id\"], inter[\"poem_id\"], inter[\"rating\"]\n",
    "                train_items = user_train.get(uid, [])\n",
    "                if not train_items:\n",
    "                    continue\n",
    "\n",
    "                try:\n",
    "                    if kind == \"cb\":\n",
    "                        rated_items = [self.items[self.pid_to_idx[t[\"poem_id\"]]] for t in train_items if t[\"poem_id\"] in self.pid_to_idx]\n",
    "                        ratings = [t[\"rating\"] for t in train_items if t[\"poem_id\"] in self.pid_to_idx]\n",
    "                        profile = rec.get_user_profile(rated_items, ratings) if rated_items else None\n",
    "                        recs = rec.recommend(profile, set(t[\"poem_id\"] for t in train_items), self.top_k) if profile else []\n",
    "                    elif kind == \"cf\":\n",
    "                        recs = rec.recommend(train_items, set(t[\"poem_id\"] for t in train_items), self.top_k)\n",
    "                    elif kind == \"enhanced_cf\":\n",
    "                        recs = rec.recommend(train_items, [], self.top_k)\n",
    "                    else:\n",
    "                        recs = []\n",
    "                except Exception as e:\n",
    "                    print(f\"{name} 推荐失败: {e}\")\n",
    "                    recs = []\n",
    "\n",
    "                exclude = set(t[\"poem_id\"] for t in train_items) | set(t[\"poem_id\"] for t in user_test.get(uid, []))\n",
    "                recs = [r for r in recs if r[\"poem_id\"] not in exclude]\n",
    "\n",
    "                predicted = [r[\"poem_id\"] for r in recs[: self.top_k]]\n",
    "                relevant = set(t[\"poem_id\"] for t in user_test[uid] if t[\"rating\"] >= self.threshold)\n",
    "\n",
    "                if not relevant:\n",
    "                    continue\n",
    "\n",
    "                tp = len(set(predicted) & relevant)\n",
    "                fp = len(predicted) - tp\n",
    "                fn = len(relevant) - tp\n",
    "\n",
    "                precision = tp / len(predicted) if predicted else 0.0\n",
    "                recall = tp / len(relevant) if relevant else 0.0\n",
    "                f1 = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0.0\n",
    "\n",
    "                p_list.append(precision)\n",
    "                r_list.append(recall)\n",
    "                f1_list.append(f1)\n",
    "                ndcg_list.append(ndcg_at_k(predicted, relevant, self.top_k))\n",
    "                mrr_list.append(mrr_at_k(predicted, relevant, self.top_k))\n",
    "\n",
    "                all_rec_items.update(predicted)\n",
    "\n",
    "            coverage = len(all_rec_items) / len(self.items) if self.items else 0\n",
    "\n",
    "            results[name] = {\n",
    "                \"precision\": {\"mean\": np.mean(p_list), \"std\": np.std(p_list)} if p_list else {\"mean\": 0.0},\n",
    "                \"recall\": {\"mean\": np.mean(r_list), \"std\": np.std(r_list)} if r_list else {\"mean\": 0.0},\n",
    "                \"f1\": {\"mean\": np.mean(f1_list), \"std\": np.std(f1_list)} if f1_list else {\"mean\": 0.0},\n",
    "                \"ndcg\": {\"mean\": np.mean(ndcg_list), \"std\": np.std(ndcg_list)} if ndcg_list else {\"mean\": 0.0},\n",
    "                \"mrr\": {\"mean\": np.mean(mrr_list), \"std\": np.std(mrr_list)} if mrr_list else {\"mean\": 0.0},\n",
    "                \"coverage\": {\"mean\": coverage},\n",
    "            }\n",
    "\n",
    "        return results\n",
    "\n",
    "\n",
    "def run_experiment(movies, ratings, test_ratio=0.2, top_k=10, threshold=3.5, seed=42):\n",
    "    \"\"\"运行单次实验\"\"\"\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    random.shuffle(ratings)\n",
    "    \n",
    "    train, test = temporal_split(ratings, test_ratio)\n",
    "    print(f\"\\n训练集: {len(train)}, 测试集: {len(test)}\")\n",
    "    \n",
    "    # 训练模型\n",
    "    print(\"\\n=== 训练Content-Based ===\")\n",
    "    cb = EnglishCB()\n",
    "    cb.fit(movies)\n",
    "    \n",
    "    print(\"\\n=== 训练Item-CF ===\")\n",
    "    cf = ItemBasedCFRecommender()\n",
    "    cf.fit(train, [m[\"id\"] for m in movies])\n",
    "    \n",
    "    print(\"\\n=== 训练BERTopic-Enhanced-CF ===\")\n",
    "    enhanced_cf = BERTopicEnhancedCF(rating_weight=0.6, topic_weight=0.4)\n",
    "    enhanced_cf.fit(movies, train)\n",
    "    \n",
    "    models = {\n",
    "        \"Content-Based\": {\"recommender\": cb, \"type\": \"cb\"},\n",
    "        \"Item-CF\": {\"recommender\": cf, \"type\": \"cf\"},\n",
    "        \"BERTopic-Enhanced-CF\": {\"recommender\": enhanced_cf, \"type\": \"enhanced_cf\"},\n",
    "    }\n",
    "    \n",
    "    evaluator = Evaluator(movies, top_k, threshold)\n",
    "    return evaluator.evaluate(models, train, test)\n",
    "\n",
    "\n",
    "def aggregate_results(all_results):\n",
    "    \"\"\"汇总多次实验结果\"\"\"\n",
    "    metrics = [\"precision\", \"recall\", \"f1\", \"ndcg\", \"mrr\", \"coverage\"]\n",
    "    out = {}\n",
    "    \n",
    "    for m in metrics:\n",
    "        values = [r[m][\"mean\"] for r in all_results if m in r]\n",
    "        if values:\n",
    "            out[m] = {\n",
    "                \"mean\": np.mean(values),\n",
    "                \"std\": np.std(values),\n",
    "                \"min\": np.min(values),\n",
    "                \"max\": np.max(values),\n",
    "            }\n",
    "    return out\n",
    "\n",
    "print(\"评估器定义完成\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==================== 运行实验 ====================\n",
    "\n",
    "n_seeds = 5\n",
    "seeds = [42, 123, 456, 789, 1024][:n_seeds]\n",
    "\n",
    "all_results = []\n",
    "\n",
    "for i, s in enumerate(seeds):\n",
    "    print(f\"\\n{'='*50}\")\n",
    "    print(f\"实验 {i+1}/{n_seeds}, Seed {s}\")\n",
    "    print('='*50)\n",
    "    \n",
    "    result = run_experiment(movies, ratings, test_ratio=0.2, top_k=10, threshold=3.5, seed=s)\n",
    "    all_results.append(result)\n",
    "\n",
    "# 汇总结果\n",
    "agg = aggregate_results(all_results)\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"实验结果汇总\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\n{'算法':<25} {'Precision':<12} {'Recall':<12} {'NDCG':<12} {'MRR':<12}\")\n",
    "print(\"-\"*60)\n",
    "\n",
    "for name in [\"Content-Based\", \"Item-CF\", \"BERTopic-Enhanced-CF\"]:\n",
    "    m = agg[name]\n",
    "    print(f\"{name:<25} {m['precision']['mean']:.4f}±{m['precision']['std']:.4f}  \"\n",
    "          f\"{m['recall']['mean']:.4f}±{m['recall']['std']:.4f}  \"\n",
    "          f\"{m['ndcg']['mean']:.4f}±{m['ndcg']['std']:.4f}  \"\n",
    "          f\"{m['mrr']['mean']:.4f}±{m['mrr']['std']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==================== 可视化结果 ====================\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 提取数据\n",
    "algorithms = [\"Content-Based\", \"Item-CF\", \"BERTopic-Enhanced-CF\"]\n",
    "metrics = [\"precision\", \"recall\", \"ndcg\", \"mrr\"]\n",
    "\n",
    "data = {}\n",
    "for alg in algorithms:\n",
    "    data[alg] = {m: agg[alg][m][\"mean\"] for m in metrics}\n",
    "\n",
    "# 绘制柱状图\n",
    "fig, axes = plt.subplots(1, 4, figsize=(16, 4))\n",
    "\n",
    "for idx, metric in enumerate(metrics):\n",
    "    ax = axes[idx]\n",
    "    values = [data[alg][metric] for alg in algorithms]\n",
    "    colors = ['#3498db', '#e74c3c', '#2ecc71']\n",
    "    bars = ax.bar(algorithms, values, color=colors)\n",
    "    ax.set_title(metric.upper(), fontsize=12)\n",
    "    ax.set_ylabel('Score')\n",
    "    \n",
    "    # 添加数值标签\n",
    "    for bar, val in zip(bars, values):\n",
    "        ax.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.001, \n",
    "                f'{val:.4f}', ha='center', va='bottom', fontsize=9)\n",
    "    \n",
    "    ax.set_ylim(0, max(values) * 1.2)\n",
    "    ax.tick_params(axis='x', rotation=15)\n",
    "\n",
    "plt.suptitle('MovieLens-100k 推荐算法对比实验', fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.savefig('experiment_results.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n图表已保存至 experiment_results.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 实验结论分析\n",
    "\n",
    "请根据上述实验结果分析：\n",
    "\n",
    "1. **BERTopic-Enhanced-CF vs Item-CF**: 主题特征的融入是否提升了推荐效果？\n",
    "2. **BERTopic-Enhanced-CF vs Content-Based**: 融合协同过滤是否优于纯内容推荐？\n",
    "3. **MRR和NDCG指标**: 排序质量是否有显著改善？"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
