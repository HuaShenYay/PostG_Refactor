é’ˆå¯¹è¯—æ­ŒLDAä¸»é¢˜æ¨¡å‹ï¼Œæˆ‘æ¨èä»¥ä¸‹**æœ€æœ‰æ•ˆä¸”ä¸å½±å“çœŸå®ä¸»é¢˜è¯**çš„å¢å¼ºæ–¹å¼ï¼š

## ğŸ¯ æ ¸å¿ƒæ¨èï¼š**åŸºäºè¯é¢‘åˆ†å¸ƒçš„æ™ºèƒ½ç‰¹å¾ç­›é€‰**

è¿™ç§æ–¹æ³•é€šè¿‡åˆ†æè¯é¢‘åˆ†å¸ƒç‰¹å¾ï¼Œç²¾å‡†å»é™¤å™ªå£°è¯ï¼Œ**ä¿ç•™çœŸæ­£çš„ä¸»é¢˜è¯**ã€‚

### ä¸€ã€åŸç†åˆ†æ
åœ¨è¯—æ­Œè¯­æ–™ä¸­ï¼ŒçœŸæ­£çš„ä¸»é¢˜è¯é€šå¸¸å…·æœ‰ä»¥ä¸‹ç‰¹å¾ï¼š
1. **ä¸­ç­‰é¢‘ç‡**ï¼šæ—¢ä¸æ˜¯æåº¦é«˜é¢‘ï¼ˆåœç”¨è¯ï¼‰ï¼Œä¹Ÿä¸æ˜¯æåº¦ä½é¢‘ï¼ˆç”Ÿåƒ»è¯ï¼‰
2. **åˆ†å¸ƒé›†ä¸­**ï¼šåœ¨æŸäº›ä¸»é¢˜æˆ–è¯—æ­Œç±»åˆ«ä¸­å‡ºç°é¢‘ç‡è¾ƒé«˜
3. **å…±ç°æ¨¡å¼**ï¼šä¸å…¶ä»–ä¸»é¢˜è¯æœ‰ç¨³å®šçš„å…±ç°å…³ç³»

### äºŒã€å…·ä½“å®ç°æ–¹æ³•

```python
import numpy as np
from collections import Counter
from sklearn.feature_extraction.text import CountVectorizer
import math

class PoetryFeatureOptimizer:
    def __init__(self, poems, min_df=0.01, max_df=0.8, coherence_threshold=0.3):
        """
        poems: è¯—æ­Œæ–‡æœ¬åˆ—è¡¨
        min_df: æœ€å°æ–‡æ¡£é¢‘ç‡æ¯”ä¾‹ï¼ˆé»˜è®¤1%ï¼‰
        max_df: æœ€å¤§æ–‡æ¡£é¢‘ç‡æ¯”ä¾‹ï¼ˆé»˜è®¤80%ï¼‰
        coherence_threshold: ä¸»é¢˜ä¸€è‡´æ€§é˜ˆå€¼
        """
        self.poems = poems
        self.min_df = min_df
        self.max_df = max_df
        self.threshold = coherence_threshold
        
    def get_optimal_features(self):
        """è·å–æœ€ä¼˜ç‰¹å¾è¯"""
        # 1. åŸºç¡€å‘é‡åŒ–
        vectorizer = CountVectorizer(min_df=2)  # å»é™¤åªå‡ºç°1æ¬¡çš„è¯
        X = vectorizer.fit_transform(self.poems)
        vocabulary = vectorizer.get_feature_names_out()
        
        # 2. è®¡ç®—è¯çš„ç»Ÿè®¡ç‰¹å¾
        word_stats = self._calculate_word_statistics(X, vocabulary)
        
        # 3. ç­›é€‰ç‰¹å¾è¯
        optimal_words = self._filter_words_by_statistics(word_stats)
        
        return optimal_words, word_stats
    
    def _calculate_word_statistics(self, X, vocabulary):
        """è®¡ç®—æ¯ä¸ªè¯çš„å¤šç§ç»Ÿè®¡æŒ‡æ ‡"""
        n_docs = X.shape[0]
        word_stats = {}
        
        # è½¬ä¸ºç¨ å¯†çŸ©é˜µä¾¿äºè®¡ç®—
        X_dense = X.toarray()
        
        for i, word in enumerate(vocabulary):
            # è¯é¢‘å‘é‡
            word_vector = X_dense[:, i]
            
            # åŸºç¡€ç»Ÿè®¡
            doc_freq = np.sum(word_vector > 0)  # å‡ºç°è¯¥è¯çš„æ–‡æ¡£æ•°
            total_freq = np.sum(word_vector)    # æ€»å‡ºç°æ¬¡æ•°
            
            # 1. æ–‡æ¡£é¢‘ç‡æ¯”ä¾‹
            df_ratio = doc_freq / n_docs
            
            # 2. é€†æ–‡æ¡£é¢‘ç‡ï¼ˆIDFï¼‰
            idf = math.log((n_docs + 1) / (doc_freq + 1)) + 1
            
            # 3. è¯é¢‘æ–¹å·®ï¼ˆè¡¡é‡åˆ†å¸ƒå‡åŒ€æ€§ï¼‰
            freq_variance = np.var(word_vector[word_vector > 0]) if doc_freq > 0 else 0
            
            # 4. è¯é¢‘ååº¦ï¼ˆè¡¡é‡åˆ†å¸ƒé›†ä¸­æ€§ï¼‰
            positive_freqs = word_vector[word_vector > 0]
            if len(positive_freqs) > 1:
                mean_freq = np.mean(positive_freqs)
                std_freq = np.std(positive_freqs)
                skewness = np.mean(((positive_freqs - mean_freq) / std_freq) ** 3) if std_freq > 0 else 0
            else:
                skewness = 0
            
            # 5. ç†µå€¼ï¼ˆè¡¡é‡è¯åœ¨æ–‡æ¡£ä¸­çš„åˆ†å¸ƒå‡åŒ€æ€§ï¼‰
            if doc_freq > 0:
                prob = word_vector / total_freq if total_freq > 0 else np.zeros_like(word_vector)
                prob_nonzero = prob[prob > 0]
                entropy = -np.sum(prob_nonzero * np.log2(prob_nonzero))
            else:
                entropy = 0
            
            word_stats[word] = {
                'doc_freq': doc_freq,
                'total_freq': total_freq,
                'df_ratio': df_ratio,
                'idf': idf,
                'freq_variance': freq_variance,
                'skewness': skewness,
                'entropy': entropy,
                'mean_freq': total_freq / doc_freq if doc_freq > 0 else 0
            }
        
        return word_stats
    
    def _filter_words_by_statistics(self, word_stats):
        """åŸºäºç»Ÿè®¡ç‰¹å¾ç­›é€‰ç‰¹å¾è¯"""
        optimal_words = []
        
        # è®¡ç®—å„æŒ‡æ ‡çš„é˜ˆå€¼ï¼ˆåŸºäºæ•°æ®åˆ†å¸ƒï¼‰
        df_ratios = [stats['df_ratio'] for stats in word_stats.values()]
        idf_values = [stats['idf'] for stats in word_stats.values()]
        entropy_values = [stats['entropy'] for stats in word_stats.values()]
        
        # åŠ¨æ€ç¡®å®šé˜ˆå€¼
        df_lower = np.percentile(df_ratios, self.min_df * 100)  # ä¸‹ç™¾åˆ†ä½
        df_upper = np.percentile(df_ratios, self.max_df * 100)  # ä¸Šç™¾åˆ†ä½
        idf_median = np.median(idf_values)
        entropy_median = np.median(entropy_values)
        
        for word, stats in word_stats.items():
            # æ’é™¤æ¡ä»¶ï¼ˆå¤§æ¦‚ç‡ä¸æ˜¯ä¸»é¢˜è¯ï¼‰
            exclude_conditions = [
                stats['df_ratio'] < df_lower,      # è¿‡äºä½é¢‘
                stats['df_ratio'] > df_upper,      # è¿‡äºé«˜é¢‘
                stats['idf'] < idf_median * 0.5,   # IDFè¿‡ä½ï¼ˆå¤ªå¸¸è§ï¼‰
                stats['entropy'] > entropy_median * 1.5,  # åˆ†å¸ƒå¤ªå‡åŒ€ï¼ˆä¸æ˜¯ä¸»é¢˜è¯ç‰¹å¾ï¼‰
                stats['skewness'] < -1,            # åˆ†å¸ƒè¿‡äºåˆ†æ•£
                len(word) == 1,                    # å•å­—è¯ï¼ˆé™¤éæ˜¯ç‰¹å®šæ„è±¡ï¼‰
                self._is_numerical_or_special(word)  # æ•°å­—æˆ–ç‰¹æ®Šå­—ç¬¦
            ]
            
            # åŒ…å«æ¡ä»¶ï¼ˆå¯èƒ½æ˜¯å¥½çš„ä¸»é¢˜è¯ï¼‰
            include_conditions = [
                df_lower <= stats['df_ratio'] <= df_upper,  # ä¸­ç­‰æ–‡æ¡£é¢‘ç‡
                stats['idf'] >= idf_median * 0.7,           # é€‚ä¸­çš„IDF
                0.5 <= stats['skewness'] <= 2,              # é€‚ä¸­çš„åˆ†å¸ƒé›†ä¸­åº¦
                stats['entropy'] <= entropy_median * 1.2,   # åˆ†å¸ƒæœ‰ä¸€å®šé›†ä¸­æ€§
                self._is_poetic_word(word)                  # è¯—æ­Œæ„è±¡è¯
            ]
            
            # åŒæ—¶æ»¡è¶³åŒ…å«æ¡ä»¶ä¸”ä¸æ»¡è¶³æ’é™¤æ¡ä»¶
            if not any(exclude_conditions) and any(include_conditions):
                # é¢å¤–åŠ æƒï¼šå¦‚æœè¯æ˜¯å¸¸è§çš„è¯—æ­Œæ„è±¡è¯ï¼Œæé«˜ä¼˜å…ˆçº§
                if self._is_common_poetic_imagery(word):
                    optimal_words.append((word, stats, 2.0))  # æƒé‡2.0
                else:
                    optimal_words.append((word, stats, 1.0))  # æƒé‡1.0
        
        # æŒ‰æ€»é¢‘ç‡å’Œæƒé‡æ’åº
        optimal_words.sort(key=lambda x: (x[1]['total_freq'] * x[2]), reverse=True)
        
        return [word for word, _, _ in optimal_words]
    
    def _is_numerical_or_special(self, word):
        """åˆ¤æ–­æ˜¯å¦ä¸ºæ•°å­—æˆ–ç‰¹æ®Šå­—ç¬¦"""
        return any(char.isdigit() for char in word) or not any(char.isalpha() for char in word)
    
    def _is_poetic_word(self, word):
        """åˆ¤æ–­æ˜¯å¦å¯èƒ½æ˜¯è¯—æ­Œè¯æ±‡ï¼ˆåŸºäºç®€å•è§„åˆ™ï¼‰"""
        poetic_keywords = {
            'è‡ªç„¶æ„è±¡': ['æœˆ', 'å±±', 'æ°´', 'äº‘', 'é£', 'é›¨', 'èŠ±', 'é›ª', 'æ±Ÿ', 'æ²³'],
            'æƒ…æ„Ÿæ„è±¡': ['æ„', 'æ€', 'æ³ª', 'å¿ƒ', 'æ¢¦', 'é­‚', 'æƒ…', 'æ¨', 'æ€¨'],
            'å­£èŠ‚æ„è±¡': ['æ˜¥', 'ç§‹', 'å¤', 'å†¬', 'å¯’', 'æš–', 'å‡‰', 'çƒ­'],
            'è‰²å½©æ„è±¡': ['çº¢', 'ç»¿', 'é’', 'ç™½', 'é»„', 'ç´«', 'ç¢§', 'ä¸¹']
        }
        
        # æ£€æŸ¥æ˜¯å¦åŒ…å«è¯—æ­Œå…³é”®è¯
        for category, keywords in poetic_keywords.items():
            if any(keyword in word for keyword in keywords):
                return True
        
        # æ£€æŸ¥æ˜¯å¦ä¸ºå¸¸è§åŒå­—è¯—æ­Œè¯æ±‡
        if len(word) == 2:
            common_poetic_bigrams = ['æ˜æœˆ', 'é’å±±', 'æµæ°´', 'æ˜¥é£', 'ç§‹é›¨', 'æ±Ÿå—', 'å¤©æ¶¯']
            if word in common_poetic_bigrams:
                return True
        
        return False
    
    def _is_common_poetic_imagery(self, word):
        """åˆ¤æ–­æ˜¯å¦ä¸ºå¸¸è§è¯—æ­Œæ„è±¡è¯"""
        common_imagery = {
            'æ˜æœˆ', 'é’å±±', 'æµæ°´', 'æ˜¥é£', 'ç§‹é›¨', 'æ±Ÿå—', 'å¤©æ¶¯', 'æ•…ä¹¡',
            'æ¨æŸ³', 'æ¡ƒèŠ±', 'æ¢§æ¡', 'èŠèŠ±', 'æ¢…èŠ±', 'è·èŠ±', 'å…°èŠ±',
            'å¤•é˜³', 'æœéœ', 'æš®è‰²', 'æ™¨æ›¦', 'å¤œç©º', 'æ˜Ÿè¾°',
            'å­¤èˆŸ', 'æ¸”ç«', 'æ¨µæ­Œ', 'ç‰§ç¬›', 'ç‚ŠçƒŸ', 'æ‘è½'
        }
        return word in common_imagery
```

### ä¸‰ã€ä¼˜åŒ–åçš„LDAè®­ç»ƒæµç¨‹

```python
class EnhancedPoetryLDA:
    def __init__(self, poems, n_topics=10):
        self.poems = poems
        self.n_topics = n_topics
        self.optimizer = PoetryFeatureOptimizer(poems)
        
    def train_optimized_lda(self):
        """è®­ç»ƒä¼˜åŒ–åçš„LDAæ¨¡å‹"""
        # 1. è·å–ä¼˜åŒ–åçš„ç‰¹å¾è¯
        optimal_words, word_stats = self.optimizer.get_optimal_features()
        print(f"åŸå§‹è¯æ±‡æ•°: {len(word_stats)}")
        print(f"ä¼˜åŒ–åè¯æ±‡æ•°: {len(optimal_words)}")
        print(f"ç‰¹å¾ä¿ç•™ç‡: {len(optimal_words)/len(word_stats)*100:.1f}%")
        
        # 2. åˆ›å»ºè‡ªå®šä¹‰å‘é‡åŒ–å™¨ï¼ˆåªä½¿ç”¨ä¼˜åŒ–åçš„è¯ï¼‰
        from sklearn.feature_extraction.text import CountVectorizer
        
        # æ„å»ºè‡ªå®šä¹‰è¯æ±‡è¡¨
        custom_vocabulary = {word: idx for idx, word in enumerate(optimal_words)}
        
        vectorizer = CountVectorizer(
            vocabulary=custom_vocabulary,
            token_pattern=r'(?u)\b\w+\b',
            max_features=len(optimal_words)
        )
        
        # 3. å‘é‡åŒ–
        X = vectorizer.fit_transform(self.poems)
        print(f"æ–‡æ¡£-è¯çŸ©é˜µå½¢çŠ¶: {X.shape}")
        
        # 4. è®¡ç®—æ¯ä¸ªç‰¹å¾çš„TF-IDFæƒé‡ï¼ˆç”¨äºåˆ†æï¼Œä¸ç”¨äºLDAï¼‰
        from sklearn.feature_extraction.text import TfidfTransformer
        tfidf_transformer = TfidfTransformer()
        X_tfidf = tfidf_transformer.fit_transform(X)
        
        # 5. åŸºäºTF-IDFè¿›ä¸€æ­¥ç­›é€‰ç‰¹å¾ï¼ˆå¯é€‰ï¼‰
        if len(optimal_words) > 1000:
            # é€‰æ‹©TF-IDFæœ€é«˜çš„ç‰¹å¾
            word_importance = np.array(X_tfidf.sum(axis=0)).flatten()
            top_indices = np.argsort(word_importance)[-1000:]  # å–å‰1000ä¸ª
            X = X[:, top_indices]
            optimal_words = [optimal_words[i] for i in top_indices]
            print(f"TF-IDFç­›é€‰åè¯æ±‡æ•°: {len(optimal_words)}")
        
        # 6. è®­ç»ƒLDA
        from sklearn.decomposition import LatentDirichletAllocation
        
        lda = LatentDirichletAllocation(
            n_components=self.n_topics,
            doc_topic_prior=0.01,  # è¾ƒå°çš„Î±ä½¿æ–‡æ¡£ä¸»é¢˜æ›´é›†ä¸­
            topic_word_prior=0.1,   # è¾ƒå°çš„Î²ä½¿ä¸»é¢˜è¯æ›´é›†ä¸­
            learning_method='online',
            random_state=42,
            max_iter=50,
            n_jobs=-1
        )
        
        lda.fit(X)
        
        # 7. è¯„ä¼°ä¸»é¢˜è´¨é‡
        self._evaluate_topics(lda, X, optimal_words)
        
        return lda, vectorizer, optimal_words
    
    def _evaluate_topics(self, model, X, feature_names):
        """è¯„ä¼°ä¸»é¢˜è´¨é‡"""
        from sklearn.metrics.pairwise import cosine_similarity
        
        # è·å–ä¸»é¢˜-è¯åˆ†å¸ƒ
        topic_word = model.components_
        
        # è®¡ç®—ä¸»é¢˜é—´ç›¸ä¼¼åº¦ï¼ˆé¿å…ä¸»é¢˜é‡å¤ï¼‰
        topic_similarity = cosine_similarity(topic_word)
        np.fill_diagonal(topic_similarity, 0)
        avg_topic_similarity = np.mean(topic_similarity)
        print(f"å¹³å‡ä¸»é¢˜é—´ç›¸ä¼¼åº¦: {avg_topic_similarity:.3f} (è¶Šä½è¶Šå¥½)")
        
        # æ˜¾ç¤ºæ¯ä¸ªä¸»é¢˜çš„å‰10ä¸ªè¯
        n_top_words = 10
        for topic_idx, topic in enumerate(model.components_):
            top_features_ind = topic.argsort()[:-n_top_words - 1:-1]
            top_features = [feature_names[i] for i in top_features_ind]
            topic_words = ' '.join(top_features)
            print(f"ä¸»é¢˜#{topic_idx}: {topic_words}")
```

### å››ã€ä½¿ç”¨ç¤ºä¾‹

```python
# å‡†å¤‡æ•°æ®
poems = [
    "åºŠå‰æ˜æœˆå…‰ï¼Œç–‘æ˜¯åœ°ä¸Šéœœã€‚ä¸¾å¤´æœ›æ˜æœˆï¼Œä½å¤´æ€æ•…ä¹¡ã€‚",
    "æ˜¥çœ ä¸è§‰æ™“ï¼Œå¤„å¤„é—»å•¼é¸Ÿã€‚å¤œæ¥é£é›¨å£°ï¼ŒèŠ±è½çŸ¥å¤šå°‘ã€‚",
    "ç™½æ—¥ä¾å±±å°½ï¼Œé»„æ²³å…¥æµ·æµã€‚æ¬²ç©·åƒé‡Œç›®ï¼Œæ›´ä¸Šä¸€å±‚æ¥¼ã€‚",
    # ... æ›´å¤šè¯—æ­Œ
]

# åˆ›å»ºå¢å¼ºLDAæ¨¡å‹
enhanced_lda = EnhancedPoetryLDA(poems, n_topics=8)

# è®­ç»ƒä¼˜åŒ–åçš„LDA
model, vectorizer, features = enhanced_lda.train_optimized_lda()

# è·å–è¯—æ­Œçš„ä¸»é¢˜åˆ†å¸ƒ
X_vectorized = vectorizer.transform(poems)
topic_distributions = model.transform(X_vectorized)

print("\nè¯—æ­Œä¸»é¢˜åˆ†å¸ƒç¤ºä¾‹:")
for i, dist in enumerate(topic_distributions[:3]):
    print(f"è¯—æ­Œ{i}: {dist}")
```

### äº”ã€ä¸ºä»€ä¹ˆè¿™ä¸ªæ–¹æ³•æœ‰æ•ˆï¼Ÿ

1. **ä¿ç•™çœŸå®ä¸»é¢˜è¯**ï¼š
   - é€šè¿‡ç»Ÿè®¡åˆ†å¸ƒç‰¹å¾è¯†åˆ«çœŸæ­£çš„ä¸»é¢˜è¯ï¼ˆä¸­ç­‰é¢‘ç‡ã€åˆ†å¸ƒé›†ä¸­ï¼‰
   - é¿å…è¿‡åº¦è¿‡æ»¤å¯¼è‡´ä¸»é¢˜ä¿¡æ¯ä¸¢å¤±

2. **å»é™¤å™ªå£°è¯**ï¼š
   - è‡ªåŠ¨è¯†åˆ«å¹¶å»é™¤æç«¯é«˜é¢‘è¯ï¼ˆé€šç”¨è¯ï¼‰å’Œæç«¯ä½é¢‘è¯ï¼ˆå™ªå£°ï¼‰
   - å»é™¤åˆ†å¸ƒè¿‡äºå‡åŒ€çš„è¯ï¼ˆè¿™äº›è¯ä¸å…·ä¸»é¢˜åŒºåˆ†æ€§ï¼‰

3. **è¯—æ­Œç‰¹æ€§è€ƒè™‘**ï¼š
   - å†…ç½®è¯—æ­Œæ„è±¡è¯è¯†åˆ«ï¼Œä¿æŠ¤æ ¸å¿ƒè¯—æ­Œè¯æ±‡
   - è€ƒè™‘åŒå­—è¯—æ­ŒçŸ­è¯­çš„é‡è¦æ€§

4. **è‡ªé€‚åº”é˜ˆå€¼**ï¼š
   - åŸºäºæ•°æ®åˆ†å¸ƒåŠ¨æ€ç¡®å®šé˜ˆå€¼ï¼Œé¿å…äººå·¥è®¾å®šåå·®
   - é€‚ç”¨äºä¸åŒè§„æ¨¡å’Œç±»å‹çš„è¯—æ­Œæ•°æ®é›†

### å…­ã€ä¸å…¶ä»–æ–¹æ³•çš„å¯¹æ¯”ä¼˜åŠ¿

| æ–¹æ³• | ä¼˜ç‚¹ | ç¼ºç‚¹ | å¯¹ä¸»é¢˜è¯å½±å“ |
|------|------|------|-------------|
| **ä¼ ç»Ÿåœç”¨è¯** | ç®€å•å¿«é€Ÿ | å¯èƒ½è¯¯åˆ ä¸»é¢˜è¯ | å¯èƒ½åˆ é™¤é‡è¦ä¸»é¢˜è¯ |
| **è¯æ€§è¿‡æ»¤** | ä¿ç•™å®è¯ | å¯èƒ½åˆ é™¤è™šè¯ä¸­çš„æƒ…æ„Ÿè¯ | å¯èƒ½åˆ é™¤æƒ…æ„Ÿä¸»é¢˜è¯ |
| **æœ¬æ–‡æ–¹æ³•** | åŸºäºç»Ÿè®¡åˆ†å¸ƒï¼Œç²¾å‡†ç­›é€‰ | è®¡ç®—å¤æ‚åº¦ç¨é«˜ | **æœ€å¤§ç¨‹åº¦ä¿ç•™çœŸå®ä¸»é¢˜è¯** |

### ä¸ƒã€è¿›ä¸€æ­¥ä¼˜åŒ–å»ºè®®

1. **ç»“åˆå¤–éƒ¨è¯å…¸**ï¼šå¯¼å…¥è¯—æ­Œæ„è±¡è¯å…¸ï¼Œå¯¹æ„è±¡è¯ç»™äºˆé¢å¤–ä¿æŠ¤æƒé‡
2. **ä½œè€…é£æ ¼è€ƒè™‘**ï¼šå¯¹ä¸åŒä½œè€…çš„è¯—æ­Œåˆ†åˆ«åˆ†æï¼Œé¿å…é£æ ¼å·®å¼‚å½±å“
3. **åŠ¨æ€æ›´æ–°**ï¼šéšç€æ–°è¯—æ­ŒåŠ å…¥ï¼ŒåŠ¨æ€æ›´æ–°ç‰¹å¾è¯åº“

è¿™ç§æ–¹æ³•**æœ€å¤§ç¨‹åº¦ä¿ç•™äº†çœŸå®çš„ä¸»é¢˜è¯**ï¼ŒåŒæ—¶æœ‰æ•ˆå»é™¤äº†å™ªå£°ï¼Œæ˜¯æå‡è¯—æ­ŒLDAä¸»é¢˜æ¨¡å‹æ•ˆæœçš„æœ€å®‰å…¨æœ‰æ•ˆæ–¹å¼ã€‚